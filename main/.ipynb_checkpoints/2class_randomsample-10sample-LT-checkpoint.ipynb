{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup GPU Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Set GPU IDs for training:\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "def build_circuit_graph_undirected(node_list,edge_list):\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(len(node_list))\n",
    "    src, dst = tuple(zip(*edge_list))\n",
    "    g.add_edges(src, dst)\n",
    "    g.add_edges(dst, src)\n",
    "    return g\n",
    "\n",
    "def build_circuit_graph_directed_sd(node_list,edge_list):\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(len(node_list))\n",
    "    src, dst = tuple(zip(*edge_list))\n",
    "    g.add_edges(src, dst)\n",
    "    return g\n",
    "\n",
    "def build_circuit_graph_directed_ds(node_list,edge_list):\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(len(node_list))\n",
    "    src, dst = tuple(zip(*edge_list))\n",
    "    g.add_edges(dst, src)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Number of Training Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30 39]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import csv\n",
    "import numpy as np\n",
    "dir = 'training_data'\n",
    "\n",
    "trainset=[]\n",
    "labels=[]\n",
    "#for filename in os.listdir(dir):\n",
    "np.random.seed(0)\n",
    "# training_idx = np.random.randint(4,64,10)\n",
    "#non-repeat generation of indices\n",
    "training_idx = np.random.choice(np.arange(4,64), size=3, replace=False)\n",
    "print(training_idx)\n",
    "# seed(0):[48 51 57  4  7 63  7 43 13 23 25 54 40 27 10 28 28 16 62  5]\n",
    "# seed(1):[41 47 16 12 13 15  9 19  4 20  5 16 11 49 10 29 54 24 41 22]\n",
    "# seed(2):[44 19 49 12 26 47 22 15 44 11 38 53 35 15 25 51 35 30 24 56]\n",
    "# seed(2):10 samples [44 19 49 12 26 47 22 15 44 11]\n",
    "# seed(0):10 samples no repeat: [30 39 63 32 15  6 38 62 44 26]\n",
    "# seed(0):5 samples no repeat: [30 39 63 32 15]\n",
    "# seed(0):3 samples no repeat: [30 39 63]\n",
    "# seed(0):2 samples no repeat: [30 39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0'], ['1'], ['0'], ['1']]\n",
      "[[0], [1], [0], [1]]\n"
     ]
    }
   ],
   "source": [
    "labels=[]\n",
    "for idx in training_idx:\n",
    "    node_list=[]\n",
    "    edge_list=[]\n",
    "    label_list=[]\n",
    "    node_list2=[]\n",
    "    edge_list2=[]\n",
    "    label_list2=[]\n",
    "    node_list3=[]\n",
    "    edge_list3=[]\n",
    "    label_list3=[]\n",
    "    node_list4=[]\n",
    "    edge_list4=[]\n",
    "    label_list4=[]\n",
    "    for j in [\"node_list\",\"edge_list\",\"graph_label\"]:\n",
    "        filename = \"cla_\"+str(idx)+\"bit\"+j+'.csv'\n",
    "        filename2 = \"CSkipA_\"+str(idx)+\"bit\"+j+'.csv'\n",
    "        if(filename.find(\"node_list\")>=0):\n",
    "            with open(dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                node_list = list(reader)\n",
    "                \n",
    "        if(filename.find(\"edge_list\")>=0):\n",
    "            with open(dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                edge_list = list(reader)\n",
    "        if(filename.find(\"graph_label\")>=0):\n",
    "            with open(dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                label_list = list(reader)\n",
    "        if(filename.find(\"gate_type\")>=0):\n",
    "            with open(dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                gate_type = list(reader)\n",
    "        \n",
    "        if(filename2.find(\"node_list\")>=0):\n",
    "            with open(dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                node_list2 = list(reader)\n",
    "                \n",
    "        if(filename2.find(\"edge_list\")>=0):\n",
    "            with open(dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                edge_list2 = list(reader)\n",
    "        if(filename2.find(\"graph_label\")>=0):\n",
    "            with open(dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                label_list2 = list(reader)\n",
    "        if(filename2.find(\"gate_type\")>=0):\n",
    "            with open(dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                gate_type2 = list(reader)\n",
    "        \n",
    "    #create dgl graph\n",
    "    g=build_circuit_graph_undirected(node_list,edge_list)\n",
    "    trainset.append(g)\n",
    "    labels.append(label_list[0])\n",
    "    g2=build_circuit_graph_undirected(node_list2,edge_list2)\n",
    "    trainset.append(g2)\n",
    "    labels.append(['1'])\n",
    "\n",
    "print(labels)\n",
    "for i in labels:\n",
    "    i[0] = int(i[0])\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph = trainset[14]\n",
    "# label=labels[14][0]\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# G=graph.to_networkx()\n",
    "# pos=nx.spring_layout(G)\n",
    "# nx.draw(G,pos)\n",
    "# nx.draw_networkx_labels(G,pos, ax=ax)\n",
    "# ax.set_title('Class: {:f}'.format(label))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply random shuffle to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##apply random shuffle on the trainset\n",
    "np.random.seed(0)\n",
    "randomize = np.arange(len(trainset))\n",
    "np.random.shuffle(randomize)\n",
    "labels_shuffled=[]\n",
    "trainset_shuffled=[]\n",
    "for i in range (len(randomize)):\n",
    "    labels_shuffled.append(labels[randomize[i]])\n",
    "    trainset_shuffled.append(trainset[randomize[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(randomize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "\n",
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Sends a message of node feature h.\n",
    "msg = fn.copy_src(src='h', out='m')\n",
    "\n",
    "def reduce(nodes):\n",
    "    \"\"\"Take an average over all neighbor node features hu and use it to\n",
    "    overwrite the original node feature.\"\"\"\n",
    "    accum = torch.mean(nodes.mailbox['m'], 1)\n",
    "    return {'h': accum}\n",
    "\n",
    "class NodeApplyModule(nn.Module):\n",
    "    \"\"\"Update the node feature hv with ReLU(Whv+b).\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        h = self.activation(h)\n",
    "        return {'h' : h}\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        # Initialize the node features with h.\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(msg, reduce)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readout and classification\n",
    "--------------------------\n",
    "For this demonstration, consider initial node features to be their degrees.\n",
    "After two rounds of graph convolution, perform a graph readout by averaging\n",
    "over all node features for each graph in the batch.\n",
    "\n",
    "\\begin{align}h_g=\\frac{1}{|\\mathcal{V}|}\\sum_{v\\in\\mathcal{V}}h_{v}\\end{align}\n",
    "\n",
    "In DGL, :func:`dgl.mean_nodes` handles this task for a batch of\n",
    "graphs with variable size. You then feed the graph representations into a\n",
    "classifier with one linear layer to obtain pre-softmax logits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            GCN(in_dim, hidden_dim, F.relu),\n",
    "            GCN(hidden_dim, hidden_dim, F.relu)])\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "        # For undirected graphs, in_degree is the same as\n",
    "        # out_degree.\n",
    "        h = g.in_degrees().view(-1, 1).float().cuda()\n",
    "        for conv in self.layers:\n",
    "            h = conv(g, h)\n",
    "        g.ndata['h'] = h\n",
    "        hg = dgl.mean_nodes(g, 'h')\n",
    "        return self.classify(hg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup and training\n",
    "------------------\n",
    "Create a synthetic dataset of $400$ graphs with $10$ ~\n",
    "$20$ nodes. $320$ graphs constitute a training set and\n",
    "$80$ graphs constitute a test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 0.9738\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 1, loss 0.7451\n",
      "Epoch 2, loss 0.7386\n",
      "Epoch 3, loss 0.7297\n",
      "Epoch 4, loss 0.7256\n",
      "Epoch 5, loss 0.7259\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, loss 0.7227\n",
      "Epoch 7, loss 0.7191\n",
      "Epoch 8, loss 0.7181\n",
      "Epoch 9, loss 0.7181\n",
      "Epoch 10, loss 0.7180\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 11, loss 0.7181\n",
      "Epoch 12, loss 0.7180\n",
      "Epoch 13, loss 0.7176\n",
      "Epoch 14, loss 0.7170\n",
      "Epoch 15, loss 0.7166\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 16, loss 0.7163\n",
      "Epoch 17, loss 0.7162\n",
      "Epoch 18, loss 0.7161\n",
      "Epoch 19, loss 0.7159\n",
      "Epoch 20, loss 0.7157\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 21, loss 0.7154\n",
      "Epoch 22, loss 0.7151\n",
      "Epoch 23, loss 0.7149\n",
      "Epoch 24, loss 0.7147\n",
      "Epoch 25, loss 0.7144\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 26, loss 0.7142\n",
      "Epoch 27, loss 0.7140\n",
      "Epoch 28, loss 0.7138\n",
      "Epoch 29, loss 0.7135\n",
      "Epoch 30, loss 0.7134\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 31, loss 0.7131\n",
      "Epoch 32, loss 0.7129\n",
      "Epoch 33, loss 0.7127\n",
      "Epoch 34, loss 0.7125\n",
      "Epoch 35, loss 0.7123\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 36, loss 0.7121\n",
      "Epoch 37, loss 0.7119\n",
      "Epoch 38, loss 0.7117\n",
      "Epoch 39, loss 0.7115\n",
      "Epoch 40, loss 0.7113\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 41, loss 0.7112\n",
      "Epoch 42, loss 0.7109\n",
      "Epoch 43, loss 0.7107\n",
      "Epoch 44, loss 0.7105\n",
      "Epoch 45, loss 0.7103\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 46, loss 0.7101\n",
      "Epoch 47, loss 0.7100\n",
      "Epoch 48, loss 0.7098\n",
      "Epoch 49, loss 0.7095\n",
      "Epoch 50, loss 0.7094\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 51, loss 0.7092\n",
      "Epoch 52, loss 0.7090\n",
      "Epoch 53, loss 0.7089\n",
      "Epoch 54, loss 0.7087\n",
      "Epoch 55, loss 0.7085\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 56, loss 0.7084\n",
      "Epoch 57, loss 0.7082\n",
      "Epoch 58, loss 0.7080\n",
      "Epoch 59, loss 0.7079\n",
      "Epoch 60, loss 0.7078\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 61, loss 0.7076\n",
      "Epoch 62, loss 0.7074\n",
      "Epoch 63, loss 0.7073\n",
      "Epoch 64, loss 0.7071\n",
      "Epoch 65, loss 0.7070\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 66, loss 0.7068\n",
      "Epoch 67, loss 0.7066\n",
      "Epoch 68, loss 0.7065\n",
      "Epoch 69, loss 0.7064\n",
      "Epoch 70, loss 0.7062\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 71, loss 0.7061\n",
      "Epoch 72, loss 0.7059\n",
      "Epoch 73, loss 0.7057\n",
      "Epoch 74, loss 0.7056\n",
      "Epoch 75, loss 0.7056\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 76, loss 0.7053\n",
      "Epoch 77, loss 0.7052\n",
      "Epoch 78, loss 0.7050\n",
      "Epoch 79, loss 0.7048\n",
      "Epoch 80, loss 0.7045\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 81, loss 0.7045\n",
      "Epoch 82, loss 0.7043\n",
      "Epoch 83, loss 0.7041\n",
      "Epoch 84, loss 0.7039\n",
      "Epoch 85, loss 0.7038\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 86, loss 0.7037\n",
      "Epoch 87, loss 0.7035\n",
      "Epoch 88, loss 0.7034\n",
      "Epoch 89, loss 0.7033\n",
      "Epoch 90, loss 0.7031\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 91, loss 0.7030\n",
      "Epoch 92, loss 0.7029\n",
      "Epoch 93, loss 0.7027\n",
      "Epoch 94, loss 0.7027\n",
      "Epoch 95, loss 0.7026\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 96, loss 0.7023\n",
      "Epoch 97, loss 0.7022\n",
      "Epoch 98, loss 0.7020\n",
      "Epoch 99, loss 0.7020\n",
      "Epoch 100, loss 0.7018\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 101, loss 0.7017\n",
      "Epoch 102, loss 0.7015\n",
      "Epoch 103, loss 0.7015\n",
      "Epoch 104, loss 0.7014\n",
      "Epoch 105, loss 0.7012\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 106, loss 0.7010\n",
      "Epoch 107, loss 0.7008\n",
      "Epoch 108, loss 0.7009\n",
      "Epoch 109, loss 0.7007\n",
      "Epoch 110, loss 0.7005\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 111, loss 0.7002\n",
      "Epoch 112, loss 0.7004\n",
      "Epoch 113, loss 0.7003\n",
      "Epoch 114, loss 0.7000\n",
      "Epoch 115, loss 0.6998\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 116, loss 0.6996\n",
      "Epoch 117, loss 0.6996\n",
      "Epoch 118, loss 0.6995\n",
      "Epoch 119, loss 0.6994\n",
      "Epoch 120, loss 0.6991\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 121, loss 0.6991\n",
      "Epoch 122, loss 0.6990\n",
      "Epoch 123, loss 0.6989\n",
      "Epoch 124, loss 0.6985\n",
      "Epoch 125, loss 0.6984\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 126, loss 0.6985\n",
      "Epoch 127, loss 0.6983\n",
      "Epoch 128, loss 0.6981\n",
      "Epoch 129, loss 0.6981\n",
      "Epoch 130, loss 0.6980\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 131, loss 0.6978\n",
      "Epoch 132, loss 0.6977\n",
      "Epoch 133, loss 0.6975\n",
      "Epoch 134, loss 0.6974\n",
      "Epoch 135, loss 0.6974\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 136, loss 0.6973\n",
      "Epoch 137, loss 0.6971\n",
      "Epoch 138, loss 0.6970\n",
      "Epoch 139, loss 0.6967\n",
      "Epoch 140, loss 0.6968\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 141, loss 0.6968\n",
      "Epoch 142, loss 0.6964\n",
      "Epoch 143, loss 0.6964\n",
      "Epoch 144, loss 0.6961\n",
      "Epoch 145, loss 0.6962\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 146, loss 0.6960\n",
      "Epoch 147, loss 0.6959\n",
      "Epoch 148, loss 0.6955\n",
      "Epoch 149, loss 0.6956\n",
      "Epoch 150, loss 0.6955\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 151, loss 0.6953\n",
      "Epoch 152, loss 0.6950\n",
      "Epoch 153, loss 0.6952\n",
      "Epoch 154, loss 0.6950\n",
      "Epoch 155, loss 0.6948\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 156, loss 0.6946\n",
      "Epoch 157, loss 0.6944\n",
      "Epoch 158, loss 0.6944\n",
      "Epoch 159, loss 0.6945\n",
      "Epoch 160, loss 0.6942\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 161, loss 0.6939\n",
      "Epoch 162, loss 0.6937\n",
      "Epoch 163, loss 0.6940\n",
      "Epoch 164, loss 0.6936\n",
      "Epoch 165, loss 0.6935\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 166, loss 0.6934\n",
      "Epoch 167, loss 0.6931\n",
      "Epoch 168, loss 0.6932\n",
      "Epoch 169, loss 0.6928\n",
      "Epoch 170, loss 0.6926\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 171, loss 0.6926\n",
      "Epoch 172, loss 0.6924\n",
      "Epoch 173, loss 0.6921\n",
      "Epoch 174, loss 0.6923\n",
      "Epoch 175, loss 0.6920\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 176, loss 0.6915\n",
      "Epoch 177, loss 0.6915\n",
      "Epoch 178, loss 0.6916\n",
      "Epoch 179, loss 0.6917\n",
      "Epoch 180, loss 0.6912\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 181, loss 0.6909\n",
      "Epoch 182, loss 0.6907\n",
      "Epoch 183, loss 0.6906\n",
      "Epoch 184, loss 0.6905\n",
      "Epoch 185, loss 0.6903\n",
      "Accuracy of argmax predictions on the training set: 75.000000%\n",
      "Epoch 186, loss 0.6902\n",
      "Epoch 187, loss 0.6899\n",
      "Epoch 188, loss 0.6897\n",
      "Epoch 189, loss 0.6894\n",
      "Epoch 190, loss 0.6895\n",
      "Accuracy of argmax predictions on the training set: 75.000000%\n",
      "Epoch 191, loss 0.6894\n",
      "Epoch 192, loss 0.6891\n",
      "Epoch 193, loss 0.6888\n",
      "Epoch 194, loss 0.6887\n",
      "Epoch 195, loss 0.6887\n",
      "Accuracy of argmax predictions on the training set: 75.000000%\n",
      "Epoch 196, loss 0.6884\n",
      "Epoch 197, loss 0.6882\n",
      "Epoch 198, loss 0.6881\n",
      "Epoch 199, loss 0.6880\n",
      "Epoch 200, loss 0.6877\n",
      "Accuracy of argmax predictions on the training set: 75.000000%\n",
      "Epoch 201, loss 0.6877\n",
      "Epoch 202, loss 0.6874\n",
      "Epoch 203, loss 0.6873\n",
      "Epoch 204, loss 0.6870\n",
      "Epoch 205, loss 0.6869\n",
      "Accuracy of argmax predictions on the training set: 75.000000%\n",
      "Epoch 206, loss 0.6869\n",
      "Epoch 207, loss 0.6865\n",
      "Epoch 208, loss 0.6863\n",
      "Epoch 209, loss 0.6866\n",
      "Epoch 210, loss 0.6860\n",
      "Accuracy of argmax predictions on the training set: 75.000000%\n",
      "Epoch 211, loss 0.6854\n",
      "Epoch 212, loss 0.6860\n",
      "Epoch 213, loss 0.6856\n",
      "Epoch 214, loss 0.6851\n",
      "Epoch 215, loss 0.6851\n",
      "Accuracy of argmax predictions on the training set: 75.000000%\n",
      "Epoch 216, loss 0.6849\n",
      "Epoch 217, loss 0.6845\n",
      "Epoch 218, loss 0.6845\n",
      "Epoch 219, loss 0.6841\n",
      "Epoch 220, loss 0.6838\n",
      "Accuracy of argmax predictions on the training set: 75.000000%\n",
      "Epoch 221, loss 0.6836\n",
      "Epoch 222, loss 0.6834\n",
      "Epoch 223, loss 0.6829\n",
      "Epoch 224, loss 0.6830\n",
      "Epoch 225, loss 0.6828\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 226, loss 0.6822\n",
      "Epoch 227, loss 0.6825\n",
      "Epoch 228, loss 0.6820\n",
      "Epoch 229, loss 0.6820\n",
      "Epoch 230, loss 0.6814\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 231, loss 0.6816\n",
      "Epoch 232, loss 0.6812\n",
      "Epoch 233, loss 0.6809\n",
      "Epoch 234, loss 0.6808\n",
      "Epoch 235, loss 0.6804\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 236, loss 0.6800\n",
      "Epoch 237, loss 0.6800\n",
      "Epoch 238, loss 0.6799\n",
      "Epoch 239, loss 0.6793\n",
      "Epoch 240, loss 0.6794\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241, loss 0.6788\n",
      "Epoch 242, loss 0.6785\n",
      "Epoch 243, loss 0.6785\n",
      "Epoch 244, loss 0.6781\n",
      "Epoch 245, loss 0.6777\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 246, loss 0.6852\n",
      "Epoch 247, loss 0.6852\n",
      "Epoch 248, loss 0.6821\n",
      "Epoch 249, loss 0.6801\n",
      "Epoch 250, loss 0.6786\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 251, loss 0.6772\n",
      "Epoch 252, loss 0.6777\n",
      "Epoch 253, loss 0.6767\n",
      "Epoch 254, loss 0.6756\n",
      "Epoch 255, loss 0.6758\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 256, loss 0.6753\n",
      "Epoch 257, loss 0.6747\n",
      "Epoch 258, loss 0.6749\n",
      "Epoch 259, loss 0.6745\n",
      "Epoch 260, loss 0.6734\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 261, loss 0.6729\n",
      "Epoch 262, loss 0.6866\n",
      "Epoch 263, loss 0.6856\n",
      "Epoch 264, loss 0.6817\n",
      "Epoch 265, loss 0.6789\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 266, loss 0.6764\n",
      "Epoch 267, loss 0.6745\n",
      "Epoch 268, loss 0.6737\n",
      "Epoch 269, loss 0.6737\n",
      "Epoch 270, loss 0.6727\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 271, loss 0.6720\n",
      "Epoch 272, loss 0.6710\n",
      "Epoch 273, loss 0.6705\n",
      "Epoch 274, loss 0.6707\n",
      "Epoch 275, loss 0.6696\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 276, loss 0.6691\n",
      "Epoch 277, loss 0.6683\n",
      "Epoch 278, loss 0.6679\n",
      "Epoch 279, loss 0.6677\n",
      "Epoch 280, loss 0.6669\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 281, loss 0.6666\n",
      "Epoch 282, loss 0.6665\n",
      "Epoch 283, loss 0.6654\n",
      "Epoch 284, loss 0.6651\n",
      "Epoch 285, loss 0.6649\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 286, loss 0.6638\n",
      "Epoch 287, loss 0.6637\n",
      "Epoch 288, loss 0.6633\n",
      "Epoch 289, loss 0.6628\n",
      "Epoch 290, loss 0.6621\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 291, loss 0.6622\n",
      "Epoch 292, loss 0.6619\n",
      "Epoch 293, loss 0.6611\n",
      "Epoch 294, loss 0.6598\n",
      "Epoch 295, loss 0.6596\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 296, loss 0.6594\n",
      "Epoch 297, loss 0.6589\n",
      "Epoch 298, loss 0.6581\n",
      "Epoch 299, loss 0.6580\n",
      "Epoch 300, loss 0.6578\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 301, loss 0.6570\n",
      "Epoch 302, loss 0.6564\n",
      "Epoch 303, loss 0.6557\n",
      "Epoch 304, loss 0.6555\n",
      "Epoch 305, loss 0.6660\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 306, loss 0.6551\n",
      "Epoch 307, loss 0.6538\n",
      "Epoch 308, loss 0.6528\n",
      "Epoch 309, loss 0.6524\n",
      "Epoch 310, loss 0.6519\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 311, loss 0.6513\n",
      "Epoch 312, loss 0.6505\n",
      "Epoch 313, loss 0.6501\n",
      "Epoch 314, loss 0.6494\n",
      "Epoch 315, loss 0.6492\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 316, loss 0.6482\n",
      "Epoch 317, loss 0.6472\n",
      "Epoch 318, loss 0.6472\n",
      "Epoch 319, loss 0.6463\n",
      "Epoch 320, loss 0.6455\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 321, loss 0.6453\n",
      "Epoch 322, loss 0.6445\n",
      "Epoch 323, loss 0.6433\n",
      "Epoch 324, loss 0.6436\n",
      "Epoch 325, loss 0.6424\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 326, loss 0.6412\n",
      "Epoch 327, loss 0.6415\n",
      "Epoch 328, loss 0.6401\n",
      "Epoch 329, loss 0.6400\n",
      "Epoch 330, loss 0.6392\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 331, loss 0.6376\n",
      "Epoch 332, loss 0.6376\n",
      "Epoch 333, loss 0.6368\n",
      "Epoch 334, loss 0.6356\n",
      "Epoch 335, loss 0.6355\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 336, loss 0.6344\n",
      "Epoch 337, loss 0.6336\n",
      "Epoch 338, loss 0.6332\n",
      "Epoch 339, loss 0.6326\n",
      "Epoch 340, loss 0.6312\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 341, loss 0.6306\n",
      "Epoch 342, loss 0.6303\n",
      "Epoch 343, loss 0.6287\n",
      "Epoch 344, loss 0.6286\n",
      "Epoch 345, loss 0.6275\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 346, loss 0.6269\n",
      "Epoch 347, loss 0.6256\n",
      "Epoch 348, loss 0.6249\n",
      "Epoch 349, loss 0.6243\n",
      "Epoch 350, loss 0.6227\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 351, loss 0.6227\n",
      "Epoch 352, loss 0.6340\n",
      "Epoch 353, loss 0.6216\n",
      "Epoch 354, loss 0.6201\n",
      "Epoch 355, loss 0.6180\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 356, loss 0.6187\n",
      "Epoch 357, loss 0.6169\n",
      "Epoch 358, loss 0.6159\n",
      "Epoch 359, loss 0.6155\n",
      "Epoch 360, loss 0.6140\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 361, loss 0.6131\n",
      "Epoch 362, loss 0.6133\n",
      "Epoch 363, loss 0.6115\n",
      "Epoch 364, loss 0.6101\n",
      "Epoch 365, loss 0.6101\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 366, loss 0.6089\n",
      "Epoch 367, loss 0.6080\n",
      "Epoch 368, loss 0.6065\n",
      "Epoch 369, loss 0.6065\n",
      "Epoch 370, loss 0.6038\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 371, loss 0.6049\n",
      "Epoch 372, loss 0.6026\n",
      "Epoch 373, loss 0.6018\n",
      "Epoch 374, loss 0.6006\n",
      "Epoch 375, loss 0.5997\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 376, loss 0.5988\n",
      "Epoch 377, loss 0.5967\n",
      "Epoch 378, loss 0.5962\n",
      "Epoch 379, loss 0.5963\n",
      "Epoch 380, loss 0.5940\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 381, loss 0.5925\n",
      "Epoch 382, loss 0.5925\n",
      "Epoch 383, loss 0.5908\n",
      "Epoch 384, loss 0.5890\n",
      "Epoch 385, loss 0.5889\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 386, loss 0.5876\n",
      "Epoch 387, loss 0.5851\n",
      "Epoch 388, loss 0.5865\n",
      "Epoch 389, loss 0.5846\n",
      "Epoch 390, loss 0.5816\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 391, loss 0.5831\n",
      "Epoch 392, loss 0.5810\n",
      "Epoch 393, loss 0.5783\n",
      "Epoch 394, loss 0.5790\n",
      "Epoch 395, loss 0.5767\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 396, loss 0.5750\n",
      "Epoch 397, loss 0.5756\n",
      "Epoch 398, loss 0.5727\n",
      "Epoch 399, loss 0.5710\n",
      "Epoch 400, loss 0.5721\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 401, loss 0.5689\n",
      "Epoch 402, loss 0.5669\n",
      "Epoch 403, loss 0.5684\n",
      "Epoch 404, loss 0.5650\n",
      "Epoch 405, loss 0.5641\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 406, loss 0.5637\n",
      "Epoch 407, loss 0.5605\n",
      "Epoch 408, loss 0.5608\n",
      "Epoch 409, loss 0.5596\n",
      "Epoch 410, loss 0.5574\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 411, loss 0.5564\n",
      "Epoch 412, loss 0.5552\n",
      "Epoch 413, loss 0.5532\n",
      "Epoch 414, loss 0.5523\n",
      "Epoch 415, loss 0.5507\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 416, loss 0.5490\n",
      "Epoch 417, loss 0.5479\n",
      "Epoch 418, loss 0.5464\n",
      "Epoch 419, loss 0.5446\n",
      "Epoch 420, loss 0.5433\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 421, loss 0.5423\n",
      "Epoch 422, loss 0.5401\n",
      "Epoch 423, loss 0.5389\n",
      "Epoch 424, loss 0.5385\n",
      "Epoch 425, loss 0.5357\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 426, loss 0.5344\n",
      "Epoch 427, loss 0.5339\n",
      "Epoch 428, loss 0.5312\n",
      "Epoch 429, loss 0.5304\n",
      "Epoch 430, loss 0.5288\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 431, loss 0.5268\n",
      "Epoch 432, loss 0.5259\n",
      "Epoch 433, loss 0.5233\n",
      "Epoch 434, loss 0.5230\n",
      "Epoch 435, loss 0.5206\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 436, loss 0.5191\n",
      "Epoch 437, loss 0.5178\n",
      "Epoch 438, loss 0.5322\n",
      "Epoch 439, loss 0.5173\n",
      "Epoch 440, loss 0.5115\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 441, loss 0.5110\n",
      "Epoch 442, loss 0.5105\n",
      "Epoch 443, loss 0.5073\n",
      "Epoch 444, loss 0.5062\n",
      "Epoch 445, loss 0.5052\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 446, loss 0.5020\n",
      "Epoch 447, loss 0.5022\n",
      "Epoch 448, loss 0.4992\n",
      "Epoch 449, loss 0.5021\n",
      "Epoch 450, loss 0.5047\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 451, loss 0.4993\n",
      "Epoch 452, loss 0.4925\n",
      "Epoch 453, loss 0.4940\n",
      "Epoch 454, loss 0.4908\n",
      "Epoch 455, loss 0.4874\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 456, loss 0.4872\n",
      "Epoch 457, loss 0.4852\n",
      "Epoch 458, loss 0.4822\n",
      "Epoch 459, loss 0.4820\n",
      "Epoch 460, loss 0.4792\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 461, loss 0.4777\n",
      "Epoch 462, loss 0.4759\n",
      "Epoch 463, loss 0.4745\n",
      "Epoch 464, loss 0.4717\n",
      "Epoch 465, loss 0.4710\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 466, loss 0.4687\n",
      "Epoch 467, loss 0.4668\n",
      "Epoch 468, loss 0.4663\n",
      "Epoch 469, loss 0.4631\n",
      "Epoch 470, loss 0.4620\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 471, loss 0.4600\n",
      "Epoch 472, loss 0.4583\n",
      "Epoch 473, loss 0.4568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 474, loss 0.4543\n",
      "Epoch 475, loss 0.4531\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 476, loss 0.4509\n",
      "Epoch 477, loss 0.4488\n",
      "Epoch 478, loss 0.4479\n",
      "Epoch 479, loss 0.4453\n",
      "Epoch 480, loss 0.4433\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 481, loss 0.4424\n",
      "Epoch 482, loss 0.4402\n",
      "Epoch 483, loss 0.4376\n",
      "Epoch 484, loss 0.4370\n",
      "Epoch 485, loss 0.4345\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 486, loss 0.4329\n",
      "Epoch 487, loss 0.4311\n",
      "Epoch 488, loss 0.4289\n",
      "Epoch 489, loss 0.4268\n",
      "Epoch 490, loss 0.4256\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 491, loss 0.4233\n",
      "Epoch 492, loss 0.4213\n",
      "Epoch 493, loss 0.4201\n",
      "Epoch 494, loss 0.4183\n",
      "Epoch 495, loss 0.4156\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 496, loss 0.4149\n",
      "Epoch 497, loss 0.4123\n",
      "Epoch 498, loss 0.4100\n",
      "Epoch 499, loss 0.4093\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "#model = Classifier(1, 256, trainset.num_classes)\n",
    "model = Classifier(1, 256, 2)\n",
    "model.cuda()\n",
    "loss_func = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "\n",
    "labels_shuffled = torch.LongTensor(labels_shuffled).cuda()\n",
    "\n",
    "epoch_losses = []\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for iter, bg in enumerate(trainset_shuffled):\n",
    "        prediction=torch.zeros(1,2,dtype=torch.float64).cuda()\n",
    "        prediction[0] = model(bg)\n",
    "\n",
    "        loss = loss_func(prediction, labels_shuffled[iter])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        #add in gradient clipping:\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "    epoch_loss /= (iter + 1)\n",
    "    print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    \n",
    "    if (epoch%5==0):\n",
    "        model.eval()\n",
    "        eval_bg = dgl.batch(trainset_shuffled)\n",
    "        eval_labels = torch.tensor(labels_shuffled).float().view(-1, 1)\n",
    "        probs_Y = torch.softmax(model(eval_bg), 1)\n",
    "        argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1)\n",
    "\n",
    "        print('Accuracy of argmax predictions on the training set: {:4f}%'.format(\n",
    "            (eval_labels == argmax_Y.float()).sum().item() / len(eval_labels) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curve of a run is presented below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('cross entropy averaged over minibatches')\n",
    "plt.plot(epoch_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Testset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]\n",
      "[30 39]\n"
     ]
    }
   ],
   "source": [
    "testset = []\n",
    "test_labels = []\n",
    "test_dir = 'test_data'\n",
    "\n",
    "testing_idx=[]\n",
    "for i in range(4,65):\n",
    "    if i not in training_idx:\n",
    "        testing_idx.append(i)\n",
    "\n",
    "print(testing_idx)\n",
    "print(training_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1]]\n"
     ]
    }
   ],
   "source": [
    "for idx in testing_idx:\n",
    "    node_list=[]\n",
    "    edge_list=[]\n",
    "    label_list=[]\n",
    "    node_list2=[]\n",
    "    edge_list2=[]\n",
    "    label_list2=[]\n",
    "    node_list3=[]\n",
    "    edge_list3=[]\n",
    "    label_list3=[]\n",
    "    node_list4=[]\n",
    "    edge_list4=[]\n",
    "    label_list4=[]\n",
    "    for j in [\"node_list\",\"edge_list\",\"graph_label\"]:\n",
    "        filename = \"cla_\"+str(idx)+\"bit\"+j+'.csv'\n",
    "        filename2 = \"CSkipA_\"+str(idx)+\"bit\"+j+'.csv'\n",
    "        \n",
    "        if(filename.find(\"node_list\")>=0):\n",
    "            with open(test_dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                node_list = list(reader)\n",
    "                \n",
    "        if(filename.find(\"edge_list\")>=0):\n",
    "            with open(test_dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                edge_list = list(reader)\n",
    "        if(filename.find(\"graph_label\")>=0):\n",
    "            with open(test_dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                label_list = list(reader)\n",
    "        if(filename.find(\"gate_type\")>=0):\n",
    "            with open(test_dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                gate_type = list(reader)\n",
    "        \n",
    "        if(filename2.find(\"node_list\")>=0):\n",
    "            with open(test_dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                node_list2 = list(reader)\n",
    "                \n",
    "        if(filename2.find(\"edge_list\")>=0):\n",
    "            with open(test_dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                edge_list2 = list(reader)\n",
    "        if(filename2.find(\"graph_label\")>=0):\n",
    "            with open(test_dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                label_list2 = list(reader)\n",
    "        if(filename2.find(\"gate_type\")>=0):\n",
    "            with open(test_dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                gate_type2 = list(reader)\n",
    "    #create dgl graph\n",
    "    g=build_circuit_graph_undirected(node_list,edge_list)\n",
    "    testset.append(g)\n",
    "    test_labels.append(label_list[0])\n",
    "    g2=build_circuit_graph_undirected(node_list2,edge_list2)\n",
    "    testset.append(g2)\n",
    "    test_labels.append(['1'])\n",
    "\n",
    "\n",
    "for i in test_labels:\n",
    "    i[0] = int(i[0])\n",
    "\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 86  85  94   8  89  22   7  10  45  68  33 110   2  51 117  73  30  43\n",
      "  84  75  62 102  16  24 109  13  63  71  26 113  50  74  54  60   3  95\n",
      "   6  56  98  90 112  48  99  76  27  18  11  61 106  97  78  59   1  92\n",
      "  42  41   4  15  17  52  40  38   5  53 108  66   0  34  28  55  35  23\n",
      "  31  93  57  91 101  32 100  14  96  19  29  49  82 115 116  79  69  80\n",
      "  20 111  72  77  25  37  81 104  46 107  39  65  58  12 105  88  70  87\n",
      "  36  21  83   9 103 114  67  64  47  44]\n"
     ]
    }
   ],
   "source": [
    "##apply random shuffle on the testset   \n",
    "np.random.seed(0)\n",
    "randomize = np.arange(len(testset))\n",
    "np.random.shuffle(randomize)\n",
    "testset_shuffled=[]\n",
    "test_labels_shuffled=[]\n",
    "for i in range (len(randomize)):\n",
    "    test_labels_shuffled.append(test_labels[randomize[i]])\n",
    "    testset_shuffled.append(testset[randomize[i]])\n",
    "print(randomize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "print(argmax_Y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of sampled predictions on the test set: 76.2712%\n",
      "Accuracy of argmax predictions on the test set: 100.000000%\n",
      "tensor([0.6281, 0.3719], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2655, 0.7345], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7073, 0.2927], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6475, 0.3525], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2647, 0.7353], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6595, 0.3405], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2949, 0.7051], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6948, 0.3052], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2721, 0.7279], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6981, 0.3019], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2790, 0.7210], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6209, 0.3791], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7868, 0.2132], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2700, 0.7300], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2631, 0.7369], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2664, 0.7336], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6565, 0.3435], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2721, 0.7279], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6250, 0.3750], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2664, 0.7336], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6301, 0.3699], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7055, 0.2945], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6498, 0.3502], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6509, 0.3491], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2636, 0.7364], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2974, 0.7026], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2685, 0.7315], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2664, 0.7336], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6503, 0.3497], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2631, 0.7369], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7056, 0.2944], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7057, 0.2943], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6926, 0.3074], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7079, 0.2921], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2949, 0.7051], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2641, 0.7359], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7458, 0.2542], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7026, 0.2974], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7071, 0.2929], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7099, 0.2901], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6195, 0.3805], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6941, 0.3059], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2641, 0.7359], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6240, 0.3760], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2790, 0.7210], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6595, 0.3405], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2974, 0.7026], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2685, 0.7315], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7085, 0.2915], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2641, 0.7359], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7038, 0.2962], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2685, 0.7315], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3516, 0.6484], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7014, 0.2986], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6624, 0.3376], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2749, 0.7251], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6960, 0.3040], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2974, 0.7026], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2974, 0.7026], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7053, 0.2947], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6574, 0.3426], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6702, 0.3298], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2949, 0.7051], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2700, 0.7300], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6183, 0.3817], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6315, 0.3685], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7309, 0.2691], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6569, 0.3431], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6338, 0.3662], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2700, 0.7300], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2749, 0.7251], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2857, 0.7143], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2790, 0.7210], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2647, 0.7353], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2685, 0.7315], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2647, 0.7353], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2641, 0.7359], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6366, 0.3634], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6144, 0.3856], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6860, 0.3140], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7014, 0.2986], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2857, 0.7143], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2790, 0.7210], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2721, 0.7279], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7076, 0.2924], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2631, 0.7369], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7189, 0.2811], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2655, 0.7345], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2673, 0.7327], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6222, 0.3778], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6504, 0.3496], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2631, 0.7369], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6983, 0.3017], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2664, 0.7336], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2857, 0.7143], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2749, 0.7251], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2655, 0.7345], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6170, 0.3830], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6617, 0.3383], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2636, 0.7364], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2749, 0.7251], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2673, 0.7327], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6266, 0.3734], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6411, 0.3589], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2636, 0.7364], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6262, 0.3738], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7059, 0.2941], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2647, 0.7353], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6544, 0.3456], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2857, 0.7143], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2655, 0.7345], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2949, 0.7051], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2636, 0.7364], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7239, 0.2761], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2673, 0.7327], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6342, 0.3658], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2721, 0.7279], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6571, 0.3429], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_bg = dgl.batch(testset_shuffled)\n",
    "test_labels = torch.tensor(test_labels_shuffled).float().view(-1, 1).cuda()\n",
    "probs_Y = torch.softmax(model(test_bg), 1)\n",
    "\n",
    "sampled_Y = torch.multinomial(probs_Y, 1)\n",
    "argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1)\n",
    "print('Accuracy of sampled predictions on the test set: {:.4f}%'.format(\n",
    "    (test_labels == sampled_Y.float()).sum().item() / len(test_labels) * 100))\n",
    "print('Accuracy of argmax predictions on the test set: {:4f}%'.format(\n",
    "    (test_labels == argmax_Y.float()).sum().item() / len(test_labels) * 100))\n",
    "\n",
    "zip(model(test_bg),(test_labels))\n",
    "for i1,i2 in zip(probs_Y,(test_labels)):\n",
    "    print(i1,i2)\n",
    "# print(torch.max(probs_Y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsne\n",
    "\n",
    "#m = trainset[0].adjacency_matrix()\n",
    "m = model(test_bg).cpu().data.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing the data using PCA...\n",
      "Computing pairwise distances...\n",
      "Computing P-values for point 0 of 118...\n",
      "Mean value of sigma: 0.062700\n",
      "Iteration 10: error is 9.371491\n",
      "Iteration 20: error is 8.515061\n",
      "Iteration 30: error is 9.682175\n",
      "Iteration 40: error is 8.768148\n",
      "Iteration 50: error is 9.136598\n",
      "Iteration 60: error is 8.557288\n",
      "Iteration 70: error is 8.249598\n",
      "Iteration 80: error is 8.147034\n",
      "Iteration 90: error is 8.714248\n",
      "Iteration 100: error is 7.975076\n",
      "Iteration 110: error is 0.933203\n",
      "Iteration 120: error is 0.569482\n",
      "Iteration 130: error is 0.322797\n",
      "Iteration 140: error is 0.177655\n",
      "Iteration 150: error is 0.082725\n",
      "Iteration 160: error is 0.070539\n",
      "Iteration 170: error is 0.062809\n",
      "Iteration 180: error is 0.049883\n",
      "Iteration 190: error is 0.031762\n",
      "Iteration 200: error is 0.031561\n",
      "Iteration 210: error is 0.030639\n",
      "Iteration 220: error is 0.030474\n",
      "Iteration 230: error is 0.030350\n",
      "Iteration 240: error is 0.030217\n",
      "Iteration 250: error is 0.030078\n",
      "Iteration 260: error is 0.029937\n",
      "Iteration 270: error is 0.029814\n",
      "Iteration 280: error is 0.029697\n",
      "Iteration 290: error is 0.029575\n",
      "Iteration 300: error is 0.029477\n",
      "Iteration 310: error is 0.029379\n",
      "Iteration 320: error is 0.029277\n",
      "Iteration 330: error is 0.029189\n",
      "Iteration 340: error is 0.029105\n",
      "Iteration 350: error is 0.029022\n",
      "Iteration 360: error is 0.028947\n",
      "Iteration 370: error is 0.028870\n",
      "Iteration 380: error is 0.028799\n",
      "Iteration 390: error is 0.028734\n",
      "Iteration 400: error is 0.028668\n",
      "Iteration 410: error is 0.028607\n",
      "Iteration 420: error is 0.028543\n",
      "Iteration 430: error is 0.028485\n",
      "Iteration 440: error is 0.028428\n",
      "Iteration 450: error is 0.028372\n",
      "Iteration 460: error is 0.028318\n",
      "Iteration 470: error is 0.028269\n",
      "Iteration 480: error is 0.028218\n",
      "Iteration 490: error is 0.028171\n",
      "Iteration 500: error is 0.028126\n",
      "Iteration 510: error is 0.028079\n",
      "Iteration 520: error is 0.028036\n",
      "Iteration 530: error is 0.027996\n",
      "Iteration 540: error is 0.027956\n",
      "Iteration 550: error is 0.027919\n",
      "Iteration 560: error is 0.027882\n",
      "Iteration 570: error is 0.027844\n",
      "Iteration 580: error is 0.027812\n",
      "Iteration 590: error is 0.027781\n",
      "Iteration 600: error is 0.027747\n",
      "Iteration 610: error is 0.027713\n",
      "Iteration 620: error is 0.027681\n",
      "Iteration 630: error is 0.027654\n",
      "Iteration 640: error is 0.027626\n",
      "Iteration 650: error is 0.027596\n",
      "Iteration 660: error is 0.027568\n",
      "Iteration 670: error is 0.027539\n",
      "Iteration 680: error is 0.027510\n",
      "Iteration 690: error is 0.027482\n",
      "Iteration 700: error is 0.027457\n",
      "Iteration 710: error is 0.027430\n",
      "Iteration 720: error is 0.027404\n",
      "Iteration 730: error is 0.027378\n",
      "Iteration 740: error is 0.027353\n",
      "Iteration 750: error is 0.027328\n",
      "Iteration 760: error is 0.027305\n",
      "Iteration 770: error is 0.027280\n",
      "Iteration 780: error is 0.027258\n",
      "Iteration 790: error is 0.027236\n",
      "Iteration 800: error is 0.027213\n",
      "Iteration 810: error is 0.027191\n",
      "Iteration 820: error is 0.027172\n",
      "Iteration 830: error is 0.027152\n",
      "Iteration 840: error is 0.027132\n",
      "Iteration 850: error is 0.027112\n",
      "Iteration 860: error is 0.027093\n",
      "Iteration 870: error is 0.027073\n",
      "Iteration 880: error is 0.027055\n",
      "Iteration 890: error is 0.027037\n",
      "Iteration 900: error is 0.027019\n",
      "Iteration 910: error is 0.027001\n",
      "Iteration 920: error is 0.026983\n",
      "Iteration 930: error is 0.026966\n",
      "Iteration 940: error is 0.026949\n",
      "Iteration 950: error is 0.026931\n",
      "Iteration 960: error is 0.026915\n",
      "Iteration 970: error is 0.026900\n",
      "Iteration 980: error is 0.026883\n",
      "Iteration 990: error is 0.026868\n",
      "Iteration 1000: error is 0.026853\n"
     ]
    }
   ],
   "source": [
    "Y = tsne.tsne(m, 2, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 13.4228394  -11.49259042]\n",
      " [-10.21815194  14.23977827]\n",
      " [  8.43736586 -15.24965017]\n",
      " [  8.78063637 -15.76906534]\n",
      " [  9.27310843 -14.73426781]\n",
      " [  8.40072757 -15.2754915 ]\n",
      " [-10.59810131  13.85570675]\n",
      " [ 12.78680992 -11.54748612]\n",
      " [  8.93477065 -15.56504416]\n",
      " [ 12.26983167 -12.38527972]\n",
      " [-12.51148492  11.9262152 ]\n",
      " [  9.55700061 -14.75301034]\n",
      " [-13.06505887  11.36833234]\n",
      " [ -9.52968842  15.02966089]\n",
      " [  8.3702075  -15.30340927]\n",
      " [ 14.23574809 -10.71807669]\n",
      " [-11.54574185  12.89988869]\n",
      " [ -9.90346911  14.55621915]\n",
      " [ -8.68757923  15.42845312]\n",
      " [  9.54863858 -14.85499713]\n",
      " [  9.54501187 -14.86053335]\n",
      " [  8.47866222 -15.87555294]\n",
      " [ 12.30528034 -12.35644408]\n",
      " [  9.7280896  -14.53890422]\n",
      " [  8.88728147 -15.64558992]\n",
      " [ -9.29156731  15.64513958]\n",
      " [  8.26430005 -15.49070259]\n",
      " [-10.21775002  14.23979359]\n",
      " [ 13.34288083 -11.74265156]\n",
      " [ 12.91721116 -11.47363963]\n",
      " [-13.59945858  10.83951361]\n",
      " [  9.26806281 -15.12627084]\n",
      " [ -9.04622642  15.04507402]\n",
      " [ -9.52968873  15.02966257]\n",
      " [-10.59827579  13.85536988]\n",
      " [-14.03876602  10.42734893]\n",
      " [-13.26553399  11.16796579]\n",
      " [ -8.68759314  15.42861042]\n",
      " [-13.59980449  10.8400907 ]\n",
      " [ -9.43504238  14.93691302]\n",
      " [-13.06485294  11.36876952]\n",
      " [ -8.70007188  15.53654194]\n",
      " [-13.26610436  11.16778143]\n",
      " [-11.98797044  12.45428207]\n",
      " [-10.21772399  14.23982   ]\n",
      " [ 13.24284406 -11.92218241]\n",
      " [ 12.6533299  -11.69610592]\n",
      " [-12.51159349  11.92580962]\n",
      " [-12.51147329  11.92536489]\n",
      " [ -9.0462276   15.04507318]\n",
      " [-11.98686786  12.45441268]\n",
      " [-12.51145816  11.92585116]\n",
      " [  9.29002669 -15.1138682 ]\n",
      " [-13.06494144  11.36794762]\n",
      " [ -8.82346226  15.18297978]\n",
      " [-10.59925131  13.85397062]\n",
      " [ 12.68655964 -11.6469998 ]\n",
      " [  8.8626112  -15.68062618]\n",
      " [  9.31837631 -14.7092216 ]\n",
      " [ -9.04920676  15.7841811 ]\n",
      " [  8.84537732 -15.0834994 ]\n",
      " [-13.0655435   11.36839429]\n",
      " [ -8.82346013  15.18298921]\n",
      " [ 12.88282105 -11.48866264]\n",
      " [ 12.55813258 -12.13407606]\n",
      " [  9.56305181 -14.64788045]\n",
      " [ 12.59551655 -12.06612834]\n",
      " [  8.72649391 -15.81001133]\n",
      " [ 14.13023188 -10.80556909]\n",
      " [  8.26314764 -15.77077857]\n",
      " [  9.658559   -14.59254958]\n",
      " [-13.26552176  11.16797799]\n",
      " [ -9.90350677  14.55618039]\n",
      " [ 13.52015161 -11.33074622]\n",
      " [  8.38761994 -15.85085398]\n",
      " [ 10.21822391 -14.12846277]\n",
      " [  8.58958838 -15.17527443]\n",
      " [ -8.94129437  15.77293943]\n",
      " [  9.17742397 -14.81904865]\n",
      " [ -9.9035328   14.55615328]\n",
      " [ 13.05707095 -12.0804355 ]\n",
      " [ -9.42673162  15.42017105]\n",
      " [-11.9868751   12.45464083]\n",
      " [ -9.04935819  15.78419154]\n",
      " [ 12.64927786 -11.70303992]\n",
      " [-10.59874822  13.85518607]\n",
      " [ 12.5889892  -11.99031994]\n",
      " [ 13.0284135  -11.44076449]\n",
      " [ 13.16003123 -12.01244191]\n",
      " [-10.21772399  14.23982   ]\n",
      " [ -9.43504344  14.93691329]\n",
      " [ 13.10543747 -12.05336382]\n",
      " [ 13.24224036 -11.9230863 ]\n",
      " [-11.06028185  13.38912229]\n",
      " [-11.54536383  12.9004119 ]\n",
      " [  8.31416259 -15.37473092]\n",
      " [ -8.70003308  15.53553484]\n",
      " [ 10.16748607 -14.17181783]\n",
      " [  8.27253827 -15.76794768]\n",
      " [  9.90783144 -14.39128603]\n",
      " [ 10.04616465 -14.27494569]\n",
      " [ -9.2912099   15.6448706 ]\n",
      " [-11.06057995  13.38930879]\n",
      " [ -9.42673205  15.42017045]\n",
      " [-11.5456179   12.89922408]\n",
      " [ 12.94369619 -12.11840487]\n",
      " [ 12.8991576  -12.12449325]\n",
      " [ 10.39875852 -13.97503457]\n",
      " [-11.54571588  12.89958291]\n",
      " [ 13.18875254 -11.43428115]\n",
      " [-11.98688442  12.45463137]\n",
      " [ 13.66990554 -11.19097435]\n",
      " [-13.59980631  10.84008889]\n",
      " [-11.06053764  13.3889804 ]\n",
      " [ -8.94131467  15.77294819]\n",
      " [ -9.90347029  14.5562179 ]\n",
      " [ 11.274595   -13.2303    ]\n",
      " [ 14.02176822 -10.89787614]\n",
      " [-13.2660664   11.16780208]\n",
      " [-13.59968716  10.83977364]]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2,2)\n",
    "\n",
    "color = test_labels.cpu()*85/255\n",
    "for i in range (len(test_labels)):\n",
    "    axs[0,0].scatter(m[i,0],m[i,1],color=(0,color[i],0))\n",
    "    axs[0,1].scatter(Y[i,0],Y[i,1],color=(0,color[i],0))\n",
    "   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "265.99px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
