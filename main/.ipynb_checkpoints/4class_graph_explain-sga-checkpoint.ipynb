{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup GPU Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch_geometric \n",
    "from torch_geometric.datasets import GeometricShapes\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import dgl\n",
    "from dgl.data import MiniGCDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from gnnexplainer.GNNExplainer import GNNExplainer\n",
    "# from torch_geometric.nn import  GNNExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Set GPU IDs for training:\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "def build_circuit_graph_undirected(node_list,edge_list):\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(len(node_list))\n",
    "    src, dst = tuple(zip(*edge_list))\n",
    "    g.add_edges(src, dst)\n",
    "    g.add_edges(dst, src)\n",
    "    return g\n",
    "\n",
    "def build_circuit_graph_directed_sd(node_list,edge_list):\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(len(node_list))\n",
    "    src, dst = tuple(zip(*edge_list))\n",
    "    g.add_edges(src, dst)\n",
    "    return g\n",
    "\n",
    "def build_circuit_graph_directed_ds(node_list,edge_list):\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(len(node_list))\n",
    "    src, dst = tuple(zip(*edge_list))\n",
    "    g.add_edges(dst, src)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Number of Training Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "training_size = 30\n",
    "training_idx = random.sample(list(range(5,65)), training_size) #exclude 4-bit adders\n",
    "test_idx = [item for item in list(range(5,65)) if item not in training_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23 58 24 11 18 44  9 61 54 39] [23 58 24 11 18 44  9 61 54 39] [23 58 24 11 18 44  9 61 54 39] [23 58 24 11 18 44  9 61 54 39]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import csv\n",
    "import numpy as np\n",
    "dir = './preprocessing/processed_training_data'\n",
    "\n",
    "trainset=[]\n",
    "labels=[]\n",
    "#for filename in os.listdir(dir):\n",
    "np.random.seed(1)\n",
    "sample_size = 10\n",
    "# training_idx = np.random.randint(4,64,10)\n",
    "training_idx_1 = np.random.choice(training_idx, size=sample_size, replace=False)\n",
    "# training_idx_2 = np.random.choice(training_idx, size=sample_size, replace=False)\n",
    "# training_idx_3 = np.random.choice(training_idx, size=sample_size, replace=False)\n",
    "# training_idx_4 = np.random.choice(training_idx, size=sample_size, replace=False)\n",
    "\n",
    "training_idx_2 = training_idx_1\n",
    "training_idx_3 = training_idx_1\n",
    "training_idx_4 = training_idx_1\n",
    "# training_idx_1 = [6]\n",
    "# training_idx_2 = [6]\n",
    "# training_idx_3 = [6]\n",
    "# training_idx_4 = [6]\n",
    "print(training_idx_1,training_idx_2,training_idx_3,training_idx_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [0], [2], [3], [1], [0], [2], [3], [1], [0], [2], [3], [1], [0], [2], [3], [1], [0], [2], [3], [1], [0], [2], [3], [1], [0], [2], [3], [1], [0], [2], [3], [1], [0], [2], [3], [1], [0], [2], [3]]\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(training_idx_1)):\n",
    "    node_list=[]\n",
    "    edge_list=[]\n",
    "    label_list=[]\n",
    "    node_list2=[]\n",
    "    edge_list2=[]\n",
    "    label_list2=[]\n",
    "    node_list3=[]\n",
    "    edge_list3=[]\n",
    "    label_list3=[]\n",
    "    node_list4=[]\n",
    "    edge_list4=[]\n",
    "    label_list4=[]\n",
    "    for j in [\"node_list\",\"edge_list\",\"graph_label\"]:\n",
    "        filename = \"rca_\"+str(training_idx_1[idx])+\"bit\"+j+'.csv'\n",
    "        filename2 = \"cla_\"+str(training_idx_2[idx])+\"bit\"+j+'.csv'\n",
    "        filename3 = \"csa_\"+str(training_idx_3[idx])+\"bit\"+j+'.csv'\n",
    "        filename4 = \"CSkipA_\"+str(training_idx_4[idx])+\"bit\"+j+'.csv'\n",
    "        if(filename.find(\"node_list\")>=0):\n",
    "            with open(dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                node_list = list(reader)\n",
    "                \n",
    "        if(filename.find(\"edge_list\")>=0):\n",
    "            with open(dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                edge_list = list(reader)\n",
    "        if(filename.find(\"graph_label\")>=0):\n",
    "            with open(dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                label_list = list(reader)\n",
    "        if(filename.find(\"gate_type\")>=0):\n",
    "            with open(dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                gate_type = list(reader)\n",
    "        \n",
    "        if(filename2.find(\"node_list\")>=0):\n",
    "            with open(dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                node_list2 = list(reader)\n",
    "                \n",
    "        if(filename2.find(\"edge_list\")>=0):\n",
    "            with open(dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                edge_list2 = list(reader)\n",
    "        if(filename2.find(\"graph_label\")>=0):\n",
    "            with open(dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                label_list2 = list(reader)\n",
    "        if(filename2.find(\"gate_type\")>=0):\n",
    "            with open(dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                gate_type2 = list(reader)\n",
    "        \n",
    "        if(filename3.find(\"node_list\")>=0):\n",
    "            with open(dir+'/'+filename3, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                node_list3 = list(reader)\n",
    "                \n",
    "        if(filename3.find(\"edge_list\")>=0):\n",
    "            with open(dir+'/'+filename3, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                edge_list3 = list(reader)\n",
    "        if(filename3.find(\"graph_label\")>=0):\n",
    "            with open(dir+'/'+filename3, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                label_list3 = list(reader)\n",
    "        if(filename3.find(\"gate_type\")>=0):\n",
    "            with open(dir+'/'+filename3, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                gate_type3 = list(reader)\n",
    "        if(filename4.find(\"node_list\")>=0):\n",
    "            with open(dir+'/'+filename4, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                node_list4 = list(reader)\n",
    "                \n",
    "        if(filename4.find(\"edge_list\")>=0):\n",
    "            with open(dir+'/'+filename4, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                edge_list4 = list(reader)\n",
    "        if(filename4.find(\"graph_label\")>=0):\n",
    "            with open(dir+'/'+filename4, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                label_list4 = list(reader)\n",
    "        if(filename4.find(\"gate_type\")>=0):\n",
    "            with open(dir+'/'+filename4, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                gate_type4 = list(reader)\n",
    "    #create dgl graph\n",
    "    g=build_circuit_graph_undirected(node_list,edge_list)\n",
    "    trainset.append(g)\n",
    "    labels.append(label_list[0])\n",
    "    g2=build_circuit_graph_undirected(node_list2,edge_list2)\n",
    "    trainset.append(g2)\n",
    "    labels.append(label_list2[0])\n",
    "    g3=build_circuit_graph_undirected(node_list3,edge_list3)\n",
    "    trainset.append(g3)\n",
    "    labels.append(label_list3[0])\n",
    "    g4=build_circuit_graph_undirected(node_list4,edge_list4)\n",
    "    trainset.append(g4)\n",
    "    labels.append(label_list4[0])\n",
    "\n",
    "for i in labels:\n",
    "    i[0] = int(i[0])\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph = trainset[14]\n",
    "# label=labels[14][0]\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# G=graph.to_networkx()\n",
    "# pos=nx.spring_layout(G)\n",
    "# nx.draw(G,pos)\n",
    "# nx.draw_networkx_labels(G,pos, ax=ax)\n",
    "# ax.set_title('Class: {:f}'.format(label))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply random shuffle to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##apply random shuffle on the trainset\n",
    "np.random.seed(0)\n",
    "randomize = np.arange(len(trainset))\n",
    "np.random.shuffle(randomize)\n",
    "labels_shuffled=[]\n",
    "trainset_shuffled=[]\n",
    "for i in range (len(randomize)):\n",
    "    labels_shuffled.append(labels[randomize[i]])\n",
    "    trainset_shuffled.append(trainset[randomize[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "\n",
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,in_dim, hidden_dim, n_classes):\n",
    "        super(Net, self).__init__()\n",
    "#         self.lin = Sequential(Linear(10, 10))\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h1 = F.relu(self.conv1(x, edge_index))\n",
    "#         h2 = F.dropout(h1, training=self.training)\n",
    "        h3 = F.relu(self.conv2(h1, edge_index))\n",
    "        y = torch.mean(h3, 0, True)\n",
    "        return self.classify(y),h1,h3,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Training Results Deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.manual_seed(0)\n",
    "# #CuDNN:\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "training_epoch = 300 #original = 1500 for 1 training sample 500 for 20 samples\n",
    "initial_lr = 0.01\n",
    "\n",
    "model_name = 'explaingraph_epoch30005-May-2020-13:32:24'\n",
    "\n",
    "# model_name = '4-class-model_10-samples_500-epoch_acc-99.500000_1-errs_12-Apr-2020-14:35:45'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SAVE_PATH = './models/' + model_name\n",
    "model = Net(1, 256, 4).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr = initial_lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "# loss = checkpoint['loss']\n",
    "print(device)\n",
    "training='new' #{'new','continue',\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f11c0cc3456f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-a5175adbbe01>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#         h2 = F.dropout(h1, training=self.training)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mh3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'h2' is not defined"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "#model = Classifier(1, 256, trainset.num_classes)\n",
    "import time\n",
    "from datetime import datetime\n",
    "if (training=='new'):\n",
    "    model = Net(1, 256, 4).to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "\n",
    "    epoch_losses = []\n",
    "    #start timer:\n",
    "    t0 = time.time()\n",
    "\n",
    "    for epoch in range(training_epoch):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for iter, g in enumerate(trainset):\n",
    "            x = g.in_degrees().view(-1, 1).float().to(device)\n",
    "\n",
    "#             x = torch.ones(g.batch_num_nodes[0],1).cuda()\n",
    "            a = g.edges()[0].tolist()\n",
    "            b = g.edges()[1].tolist()\n",
    "            edges = list(zip(a,b))\n",
    "            edges = np.array(edges)\n",
    "            edges = torch.LongTensor(edges.transpose()).to(device)\n",
    "            label = labels[iter]\n",
    "\n",
    "            prediction,h1,h2,h3 = model(x,edges)\n",
    "\n",
    "            loss = loss_func(prediction, torch.LongTensor(label).cuda())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "    #         print('loss {:.4f},label:{}'.format(loss,label))\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().item()\n",
    "        epoch_loss /= (iter + 1)\n",
    "        print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n",
    "        epoch_losses.append(epoch_loss)\n",
    "\n",
    "    training_time = time.time() - t0\n",
    "\n",
    "    print('Finished training. Training time = {:.4f} Seconds'.format(training_time))\n",
    "\n",
    "\n",
    "    dateTimeObj = datetime.now()\n",
    "\n",
    "    timestampStr = dateTimeObj.strftime(\"%d-%b-%Y-%H:%M:%S\")\n",
    "    model_name = '4_class_explaingraph_epoch{}'.format(training_epoch)+timestampStr\n",
    "\n",
    "    SAVE_PATH = './models/' + model_name\n",
    "    torch.save({'epoch':epoch,'model_state_dict':model.state_dict(),'optimizer_state_dict': optimizer.state_dict()}, SAVE_PATH)\n",
    "    plt.title('cross entropy averaged over minibatches')\n",
    "    plt.plot(epoch_losses)\n",
    "    plt.show()\n",
    "if(training=='continue'):\n",
    "    checkpoint = torch.load(SAVE_PATH)\n",
    "    checkpoint = torch.load(SAVE_PATH)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    for epoch in range(training_epoch):\n",
    "        epoch += start_epoch\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for iter, g in enumerate(traindata):\n",
    "            x = g.in_degrees().view(-1, 1).float().to(device)\n",
    "\n",
    "#             x = torch.ones(g.batch_num_nodes[0],1).cuda()\n",
    "            a = g.edges()[0].tolist()\n",
    "            b = g.edges()[1].tolist()\n",
    "            edges = list(zip(a,b))\n",
    "            edges = np.array(edges)\n",
    "            edges = torch.LongTensor(edges.transpose()).to(device)\n",
    "            labels = labels_shuffled[iter]\n",
    "\n",
    "            prediction,h1,h2,h3 = model(x,edges)\n",
    "            \n",
    "            loss = loss_func(prediction, torch.LongTensor(labels).cuda())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "    #         print('loss {:.4f},label:{}'.format(loss,label))\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().item()\n",
    "        epoch_loss /= (iter + 1)\n",
    "        print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    torch.save({'epoch':epoch,'model_state_dict':model.state_dict(),'optimizer_state_dict': optimizer.state_dict()}, SAVE_PATH)\n",
    "    plt.title('cross entropy averaged over minibatches')\n",
    "    plt.plot(epoch_losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('cross entropy averaged over minibatches')\n",
    "plt.plot(epoch_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layers[0].apply_mod.linear.weight.size())\n",
    "print(model.layers[1].apply_mod.linear.weight.size())\n",
    "model.classify.weight.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Testset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = []\n",
    "test_labels = []\n",
    "test_dir = dir\n",
    "\n",
    "testing_idx_1=[]\n",
    "testing_idx_2=[]\n",
    "testing_idx_3=[]\n",
    "testing_idx_4=[]\n",
    "for i in test_idx:\n",
    "    testing_idx_1.append(i)\n",
    "    testing_idx_2.append(i)\n",
    "    testing_idx_3.append(i)\n",
    "    testing_idx_4.append(i)\n",
    "      \n",
    "# print(testing_idx_1)\n",
    "# print(training_idx_1)\n",
    "# print(testing_idx_2)\n",
    "# print(training_idx_2)\n",
    "# print(testing_idx_3)\n",
    "# print(training_idx_3)\n",
    "# print(testing_idx_4)\n",
    "# print(training_idx_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sample Nodes and Edges for Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Percentage Keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_keep = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(testing_idx_1)):\n",
    "    node_list=[]\n",
    "    edge_list=[]\n",
    "    label_list=[]\n",
    "    node_list2=[]\n",
    "    edge_list2=[]\n",
    "    label_list2=[]\n",
    "    node_list3=[]\n",
    "    edge_list3=[]\n",
    "    label_list3=[]\n",
    "    node_list4=[]\n",
    "    edge_list4=[]\n",
    "    label_list4=[]\n",
    "    for j in [\"node_list\",\"edge_list\",\"graph_label\"]:\n",
    "        filename = \"rca_\"+str(testing_idx_1[idx])+\"bit\"+j+'.csv'\n",
    "        filename2 = \"cla_\"+str(testing_idx_2[idx])+\"bit\"+j+'.csv'\n",
    "        filename3 = \"csa_\"+str(testing_idx_3[idx])+\"bit\"+j+'.csv'\n",
    "        filename4 = \"CSkipA_\"+str(testing_idx_4[idx])+\"bit\"+j+'.csv'\n",
    "        \n",
    "        if(filename.find(\"node_list\")>=0):\n",
    "            with open(test_dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                node_list = list(reader)\n",
    "                \n",
    "        if(filename.find(\"edge_list\")>=0):\n",
    "            with open(test_dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                edge_list = list(reader)\n",
    "        if(filename.find(\"graph_label\")>=0):\n",
    "            with open(test_dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                label_list = list(reader)\n",
    "        if(filename.find(\"gate_type\")>=0):\n",
    "            with open(test_dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                gate_type = list(reader)\n",
    "        \n",
    "        if(filename2.find(\"node_list\")>=0):\n",
    "            with open(test_dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                node_list2 = list(reader)\n",
    "                \n",
    "        if(filename2.find(\"edge_list\")>=0):\n",
    "            with open(test_dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                edge_list2 = list(reader)\n",
    "        if(filename2.find(\"graph_label\")>=0):\n",
    "            with open(test_dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                label_list2 = list(reader)\n",
    "        if(filename2.find(\"gate_type\")>=0):\n",
    "            with open(test_dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                gate_type2 = list(reader)\n",
    "        if(filename3.find(\"node_list\")>=0):\n",
    "            with open(test_dir+'/'+filename3, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                node_list3 = list(reader)\n",
    "                \n",
    "        if(filename3.find(\"edge_list\")>=0):\n",
    "            with open(test_dir+'/'+filename3, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                edge_list3 = list(reader)\n",
    "        if(filename3.find(\"graph_label\")>=0):\n",
    "            with open(test_dir+'/'+filename3, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                label_list3 = list(reader)\n",
    "        if(filename3.find(\"gate_type\")>=0):\n",
    "            with open(test_dir+'/'+filename3, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                gate_type3 = list(reader)\n",
    "        if(filename4.find(\"node_list\")>=0):\n",
    "            with open(dir+'/'+filename4, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                node_list4 = list(reader)\n",
    "                \n",
    "        if(filename4.find(\"edge_list\")>=0):\n",
    "            with open(dir+'/'+filename4, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                edge_list4 = list(reader)\n",
    "        if(filename4.find(\"graph_label\")>=0):\n",
    "            with open(dir+'/'+filename4, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                label_list4 = list(reader)\n",
    "        if(filename4.find(\"gate_type\")>=0):\n",
    "            with open(dir+'/'+filename4, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                gate_type4 = list(reader)\n",
    "    \n",
    "    #randomly sample edges but keep all nodes\n",
    "    \n",
    "#     node_list_keep = random.sample(node_list,int(len(node_list)*pct_keep))\n",
    "    node_list_keep = node_list\n",
    "    edge_list_keep = random.sample(edge_list,int(len(edge_list)*pct_keep))\n",
    "#     node_list2_keep = random.sample(node_list2,int(len(node_list2)*pct_keep))\n",
    "    node_list2_keep = node_list2\n",
    "    edge_list2_keep = random.sample(edge_list2,int(len(edge_list2)*pct_keep))\n",
    "#     node_list3_keep = random.sample(node_list3,int(len(node_list3)*pct_keep))\n",
    "    node_list3_keep = node_list3\n",
    "    edge_list3_keep = random.sample(edge_list3,int(len(edge_list3)*pct_keep))\n",
    "#     node_list4_keep = random.sample(node_list4,int(len(node_list4)*pct_keep))\n",
    "    node_list4_keep = node_list4\n",
    "    edge_list4_keep = random.sample(edge_list4,int(len(edge_list4)*pct_keep))\n",
    "    \n",
    "    #create dgl graph\n",
    "    g=build_circuit_graph_undirected(node_list_keep,edge_list_keep)\n",
    "    testset.append(g)\n",
    "    test_labels.append(label_list[0])\n",
    "    g2=build_circuit_graph_undirected(node_list2_keep,edge_list2_keep)\n",
    "    testset.append(g2)\n",
    "    test_labels.append(label_list2[0])\n",
    "    g3=build_circuit_graph_undirected(node_list3_keep,edge_list3_keep)\n",
    "    testset.append(g3)\n",
    "    test_labels.append(label_list3[0])\n",
    "    g4=build_circuit_graph_undirected(node_list4_keep,edge_list4_keep)\n",
    "    testset.append(g4)\n",
    "    test_labels.append(label_list4[0])\n",
    "\n",
    "for i in test_labels:\n",
    "    i[0] = int(i[0])\n",
    "\n",
    "# print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##apply random shuffle on the testset    \n",
    "np.random.seed(0)\n",
    "randomize = np.arange(len(testset))\n",
    "np.random.shuffle(randomize)\n",
    "testset_shuffled=[]\n",
    "test_labels_shuffled=[]\n",
    "for i in range (len(randomize)):\n",
    "    test_labels_shuffled.append(test_labels[randomize[i]])\n",
    "    testset_shuffled.append(testset[randomize[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_bg = dgl.batch(testset_shuffled)\n",
    "test_labels_shuffled = torch.tensor(test_labels_shuffled).float().view(-1, 1).cuda()\n",
    "probs_Y = torch.softmax(model(test_bg), 1)\n",
    "\n",
    "sampled_Y = torch.multinomial(probs_Y, 1)\n",
    "argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1)\n",
    "print('Accuracy of sampled predictions on the test set: {:.4f}%'.format(\n",
    "    (test_labels_shuffled == sampled_Y.float()).sum().item() / len(test_labels) * 100))\n",
    "print('Accuracy of argmax predictions on the test set: {:.4f}%'.format(\n",
    "    (test_labels_shuffled == argmax_Y.float()).sum().item() / len(test_labels) * 100))\n",
    "\n",
    "# zip(model(test_bg),(test_labels))\n",
    "# for i1,i2 in zip(probs_Y,(test_labels)):\n",
    "#     print(i1,i2)\n",
    "# print(torch.max(probs_Y, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Error Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = (test_labels_shuffled != argmax_Y.float()).nonzero()\n",
    "print('Total', len(error),'errors out of',len(test_idx)*4,'test data')\n",
    "for i in error.cpu().numpy():\n",
    "    error_idx= i[0]\n",
    "    print(probs_Y[error_idx],test_labels_shuffled[error_idx])\n",
    "#     print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = '{:4f}'.format((test_labels_shuffled == argmax_Y.float()).sum().item() / len(test_labels) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dateTimeObj = datetime.now()\n",
    " \n",
    "timestampStr = dateTimeObj.strftime(\"%d-%b-%Y-%H:%M:%S\")\n",
    " \n",
    "print('Current Timestamp : ', timestampStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'4-class-model_{}-samples_{}-epoch_acc-'.format(sample_size,training_epoch)+test_accuracy+'_'+'{}-errs_'.format(len(error))+timestampStr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set model name:\n",
    "model_name = '4-class-model_{}-samples_{}-epoch_acc-'.format(sample_size,training_epoch)+test_accuracy+'_'+'{}-errs_'.format(len(error))+timestampStr\n",
    "\n",
    "SAVE_PATH = '/home/gpu-user-02/hdd/LT-user/Adder_Classification/main/models/' + model_name\n",
    "torch.save(model.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = '4class_1Sample_97.580645_0411'\n",
    "\n",
    "SAVE_PATH = 'models/4-class-model_10-samples_500-epoch_acc-99.500000_1-errs_12-Apr-2020-14:35:45'\n",
    "model = Classifier(1, 256, 4)\n",
    "model.load_state_dict(torch.load(SAVE_PATH))\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (test_labels != argmax_Y.float()).nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCN1_act = []\n",
    "GCN2_act = []\n",
    "graph_emb = []\n",
    "final_class = []\n",
    "\n",
    "def get_activation_1(self,input,output):\n",
    "#     print(output.size())\n",
    "    GCN1_act.append(output.data.cpu().numpy())\n",
    "    \n",
    "def get_activation_2(self,input,output):\n",
    "#     print(output.size())\n",
    "    GCN2_act.append(output.data.cpu().numpy())\n",
    "    \n",
    "def get_activation_3(self,input,output):\n",
    "#     print(output.size())\n",
    "    graph_emb.append(input[0].cpu().detach().numpy())\n",
    "    final_class.append(output.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = model.layers[0].register_forward_hook(get_activation_1)\n",
    "h2 = model.layers[1].register_forward_hook(get_activation_2)\n",
    "h3 = model.classify.register_forward_hook(get_activation_3)\n",
    "\n",
    "for i in range(len(testset)):\n",
    "    out = model(testset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(GCN1_act),len(GCN2_act),len(graph_emb),len(final_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCN1_act_vis_ls = []\n",
    "GCN2_act_vis_ls = []\n",
    "graph_emb_vis_ls = []\n",
    "final_class_vis_ls = []\n",
    "\n",
    "for i in range(len(GCN1_act)):\n",
    "    GCN1_act_vis_ls.append(np.expand_dims(np.mean(GCN1_act[i],axis=0),axis=0))\n",
    "    GCN2_act_vis_ls.append(np.expand_dims(np.mean(GCN2_act[i],axis=0),axis=0))\n",
    "    graph_emb_vis_ls.append(np.expand_dims(graph_emb[i],axis=0))\n",
    "    final_class_vis_ls.append(np.expand_dims(final_class[i],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCN1_act_vis = np.concatenate(GCN1_act_vis_ls, axis=0)\n",
    "GCN2_act_vis = np.concatenate(GCN2_act_vis_ls, axis=0)\n",
    "graph_emb_vis = np.concatenate(graph_emb_vis_ls, axis=0)\n",
    "final_class_vis = np.concatenate(final_class_vis_ls, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCN1_act_vis.shape,GCN2_act_vis.shape,graph_emb_vis.shape,final_class_vis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN1_act[0].size(),GCN2_act[0].size(),graph_emb[0].size(),final_class[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsne\n",
    "vis1 = tsne.tsne(GCN1_act_vis, 2, 30)\n",
    "vis2 = tsne.tsne(GCN2_act_vis, 2, 30)\n",
    "vis3 = tsne.tsne(graph_emb_vis, 2, 30)\n",
    "vis4 = tsne.tsne(final_class_vis, 2, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2,figsize=(15,15))\n",
    "\n",
    "# color = test_labels.cpu()*85/255\n",
    "\n",
    "#class 0: cla 'green'\n",
    "#class 1: rca 'red'\n",
    "#class 2: csla 'blue'\n",
    "#class 3: cska 'gold'\n",
    "\n",
    "color = []\n",
    "# marker = []\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i] == [0]:\n",
    "        color.append('green')\n",
    "#         marker.append('.')\n",
    "    if test_labels[i] == [1]:\n",
    "        color.append('red')\n",
    "#         marker.append('^')\n",
    "    if test_labels[i] == [2]:\n",
    "        color.append('blue')\n",
    "#         marker.append('v')\n",
    "    if test_labels[i] == [3]:\n",
    "        color.append('gold')\n",
    "#         marker.append('s')\n",
    "\n",
    "axs[0,0].set_title('GCN1')\n",
    "axs[0,1].set_title('GCN2')\n",
    "axs[1,0].set_title('Graph_emb')\n",
    "axs[1,1].set_title('Final_class')\n",
    "for i in range (len(test_labels)):\n",
    "#     axs[0,0].scatter(vis1[i,0],vis1[i,1],color=(0,color[i],0))\n",
    "#     axs[0,1].scatter(vis2[i,0],vis2[i,1],color=(0,color[i],0))\n",
    "#     axs[1,0].scatter(vis3[i,0],vis3[i,1],color=(0,color[i],0))\n",
    "#     axs[1,1].scatter(vis4[i,0],vis4[i,1],color=(0,color[i],0))\n",
    "    axs[0,0].scatter(vis1[i,0],vis1[i,1],color=color[i])\n",
    "    axs[0,1].scatter(vis2[i,0],vis2[i,1],color=color[i])\n",
    "    axs[1,0].scatter(vis3[i,0],vis3[i,1],color=color[i])\n",
    "    axs[1,1].scatter(vis4[i,0],vis4[i,1],color=color[i])\n",
    "#     axs[0,0].scatter(vis1[i,0],vis1[i,1],color=color[i],marker=marker[i])\n",
    "#     axs[0,1].scatter(vis2[i,0],vis2[i,1],color=color[i],marker=marker[i])\n",
    "#     axs[1,0].scatter(vis3[i,0],vis3[i,1],color=color[i],marker=marker[i])\n",
    "#     axs[1,1].scatter(vis4[i,0],vis4[i,1],color=color[i],marker=marker[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "266.818px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
