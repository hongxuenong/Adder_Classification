{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup GPU Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Set GPU IDs for training:\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "def build_circuit_graph_undirected(node_list,edge_list):\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(len(node_list))\n",
    "    src, dst = tuple(zip(*edge_list))\n",
    "    g.add_edges(src, dst)\n",
    "    g.add_edges(dst, src)\n",
    "    return g\n",
    "\n",
    "def build_circuit_graph_directed_sd(node_list,edge_list):\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(len(node_list))\n",
    "    src, dst = tuple(zip(*edge_list))\n",
    "    g.add_edges(src, dst)\n",
    "    return g\n",
    "\n",
    "def build_circuit_graph_directed_ds(node_list,edge_list):\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(len(node_list))\n",
    "    src, dst = tuple(zip(*edge_list))\n",
    "    g.add_edges(dst, src)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Number of Training Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30 39 63]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import csv\n",
    "import numpy as np\n",
    "dir = 'training_data'\n",
    "\n",
    "trainset=[]\n",
    "labels=[]\n",
    "#for filename in os.listdir(dir):\n",
    "np.random.seed(0)\n",
    "# training_idx = np.random.randint(4,64,10)\n",
    "#non-repeat generation of indices\n",
    "training_idx = np.random.choice(np.arange(4,64), size=3, replace=False)\n",
    "print(training_idx)\n",
    "# seed(0):[48 51 57  4  7 63  7 43 13 23 25 54 40 27 10 28 28 16 62  5]\n",
    "# seed(1):[41 47 16 12 13 15  9 19  4 20  5 16 11 49 10 29 54 24 41 22]\n",
    "# seed(2):[44 19 49 12 26 47 22 15 44 11 38 53 35 15 25 51 35 30 24 56]\n",
    "# seed(2):10 samples [44 19 49 12 26 47 22 15 44 11]\n",
    "# seed(0):10 samples no repeat: [30 39 63 32 15  6 38 62 44 26]\n",
    "# seed(0):5 samples no repeat: [30 39 63 32 15]\n",
    "# seed(0):3 samples no repeat: [30 39 63]\n",
    "# seed(0):2 samples no repeat: [30 39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0'], ['1'], ['0'], ['1'], ['0'], ['1']]\n",
      "[[0], [1], [0], [1], [0], [1]]\n"
     ]
    }
   ],
   "source": [
    "labels=[]\n",
    "for idx in training_idx:\n",
    "    node_list=[]\n",
    "    edge_list=[]\n",
    "    label_list=[]\n",
    "    node_list2=[]\n",
    "    edge_list2=[]\n",
    "    label_list2=[]\n",
    "    node_list3=[]\n",
    "    edge_list3=[]\n",
    "    label_list3=[]\n",
    "    node_list4=[]\n",
    "    edge_list4=[]\n",
    "    label_list4=[]\n",
    "    for j in [\"node_list\",\"edge_list\",\"graph_label\"]:\n",
    "        filename = \"cla_\"+str(idx)+\"bit\"+j+'.csv'\n",
    "        filename2 = \"CSkipA_\"+str(idx)+\"bit\"+j+'.csv'\n",
    "        if(filename.find(\"node_list\")>=0):\n",
    "            with open(dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                node_list = list(reader)\n",
    "                \n",
    "        if(filename.find(\"edge_list\")>=0):\n",
    "            with open(dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                edge_list = list(reader)\n",
    "        if(filename.find(\"graph_label\")>=0):\n",
    "            with open(dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                label_list = list(reader)\n",
    "        if(filename.find(\"gate_type\")>=0):\n",
    "            with open(dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                gate_type = list(reader)\n",
    "        \n",
    "        if(filename2.find(\"node_list\")>=0):\n",
    "            with open(dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                node_list2 = list(reader)\n",
    "                \n",
    "        if(filename2.find(\"edge_list\")>=0):\n",
    "            with open(dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                edge_list2 = list(reader)\n",
    "        if(filename2.find(\"graph_label\")>=0):\n",
    "            with open(dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                label_list2 = list(reader)\n",
    "        if(filename2.find(\"gate_type\")>=0):\n",
    "            with open(dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                gate_type2 = list(reader)\n",
    "        \n",
    "    #create dgl graph\n",
    "    g=build_circuit_graph_undirected(node_list,edge_list)\n",
    "    trainset.append(g)\n",
    "    labels.append(label_list[0])\n",
    "    g2=build_circuit_graph_undirected(node_list2,edge_list2)\n",
    "    trainset.append(g2)\n",
    "    labels.append(['1'])\n",
    "\n",
    "print(labels)\n",
    "for i in labels:\n",
    "    i[0] = int(i[0])\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph = trainset[14]\n",
    "# label=labels[14][0]\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# G=graph.to_networkx()\n",
    "# pos=nx.spring_layout(G)\n",
    "# nx.draw(G,pos)\n",
    "# nx.draw_networkx_labels(G,pos, ax=ax)\n",
    "# ax.set_title('Class: {:f}'.format(label))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply random shuffle to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##apply random shuffle on the trainset\n",
    "np.random.seed(0)\n",
    "randomize = np.arange(len(trainset))\n",
    "np.random.shuffle(randomize)\n",
    "labels_shuffled=[]\n",
    "trainset_shuffled=[]\n",
    "for i in range (len(randomize)):\n",
    "    labels_shuffled.append(labels[randomize[i]])\n",
    "    trainset_shuffled.append(trainset[randomize[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 2 1 3 0 4]\n"
     ]
    }
   ],
   "source": [
    "print(randomize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "\n",
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Sends a message of node feature h.\n",
    "msg = fn.copy_src(src='h', out='m')\n",
    "\n",
    "def reduce(nodes):\n",
    "    \"\"\"Take an average over all neighbor node features hu and use it to\n",
    "    overwrite the original node feature.\"\"\"\n",
    "    accum = torch.mean(nodes.mailbox['m'], 1)\n",
    "    return {'h': accum}\n",
    "\n",
    "class NodeApplyModule(nn.Module):\n",
    "    \"\"\"Update the node feature hv with ReLU(Whv+b).\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        h = self.activation(h)\n",
    "        return {'h' : h}\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        # Initialize the node features with h.\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(msg, reduce)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readout and classification\n",
    "--------------------------\n",
    "For this demonstration, consider initial node features to be their degrees.\n",
    "After two rounds of graph convolution, perform a graph readout by averaging\n",
    "over all node features for each graph in the batch.\n",
    "\n",
    "\\begin{align}h_g=\\frac{1}{|\\mathcal{V}|}\\sum_{v\\in\\mathcal{V}}h_{v}\\end{align}\n",
    "\n",
    "In DGL, :func:`dgl.mean_nodes` handles this task for a batch of\n",
    "graphs with variable size. You then feed the graph representations into a\n",
    "classifier with one linear layer to obtain pre-softmax logits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            GCN(in_dim, hidden_dim, F.relu),\n",
    "            GCN(hidden_dim, hidden_dim, F.relu)])\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "        # For undirected graphs, in_degree is the same as\n",
    "        # out_degree.\n",
    "        h = g.in_degrees().view(-1, 1).float().cuda()\n",
    "        for conv in self.layers:\n",
    "            h = conv(g, h)\n",
    "        g.ndata['h'] = h\n",
    "        hg = dgl.mean_nodes(g, 'h')\n",
    "        return self.classify(hg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup and training\n",
    "------------------\n",
    "Create a synthetic dataset of $400$ graphs with $10$ ~\n",
    "$20$ nodes. $320$ graphs constitute a training set and\n",
    "$80$ graphs constitute a test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 0.9953\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 1, loss 0.6796\n",
      "Epoch 2, loss 0.7923\n",
      "Epoch 3, loss 0.7599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, loss 0.7177\n",
      "Epoch 5, loss 0.7059\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 6, loss 0.7266\n",
      "Epoch 7, loss 0.7299\n",
      "Epoch 8, loss 0.7211\n",
      "Epoch 9, loss 0.7148\n",
      "Epoch 10, loss 0.7174\n",
      "Accuracy of argmax predictions on the training set: 50.000000%\n",
      "Epoch 11, loss 0.7204\n",
      "Epoch 12, loss 0.7194\n",
      "Epoch 13, loss 0.7169\n",
      "Epoch 14, loss 0.7160\n",
      "Epoch 15, loss 0.7163\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 16, loss 0.7165\n",
      "Epoch 17, loss 0.7158\n",
      "Epoch 18, loss 0.7147\n",
      "Epoch 19, loss 0.7141\n",
      "Epoch 20, loss 0.7142\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 21, loss 0.7138\n",
      "Epoch 22, loss 0.7131\n",
      "Epoch 23, loss 0.7123\n",
      "Epoch 24, loss 0.7123\n",
      "Epoch 25, loss 0.7120\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 26, loss 0.7112\n",
      "Epoch 27, loss 0.7106\n",
      "Epoch 28, loss 0.7105\n",
      "Epoch 29, loss 0.7103\n",
      "Epoch 30, loss 0.7096\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 31, loss 0.7089\n",
      "Epoch 32, loss 0.7088\n",
      "Epoch 33, loss 0.7087\n",
      "Epoch 34, loss 0.7080\n",
      "Epoch 35, loss 0.7071\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 36, loss 0.7072\n",
      "Epoch 37, loss 0.7071\n",
      "Epoch 38, loss 0.7064\n",
      "Epoch 39, loss 0.7055\n",
      "Epoch 40, loss 0.7057\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 41, loss 0.7054\n",
      "Epoch 42, loss 0.7049\n",
      "Epoch 43, loss 0.7041\n",
      "Epoch 44, loss 0.7042\n",
      "Epoch 45, loss 0.7041\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 46, loss 0.7035\n",
      "Epoch 47, loss 0.7028\n",
      "Epoch 48, loss 0.7025\n",
      "Epoch 49, loss 0.7025\n",
      "Epoch 50, loss 0.7021\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 51, loss 0.7015\n",
      "Epoch 52, loss 0.7013\n",
      "Epoch 53, loss 0.7010\n",
      "Epoch 54, loss 0.7004\n",
      "Epoch 55, loss 0.7003\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 56, loss 0.7001\n",
      "Epoch 57, loss 0.6996\n",
      "Epoch 58, loss 0.6993\n",
      "Epoch 59, loss 0.6989\n",
      "Epoch 60, loss 0.6985\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 61, loss 0.6984\n",
      "Epoch 62, loss 0.6980\n",
      "Epoch 63, loss 0.6977\n",
      "Epoch 64, loss 0.6972\n",
      "Epoch 65, loss 0.6970\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 66, loss 0.6969\n",
      "Epoch 67, loss 0.6963\n",
      "Epoch 68, loss 0.6961\n",
      "Epoch 69, loss 0.6960\n",
      "Epoch 70, loss 0.6956\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 71, loss 0.6949\n",
      "Epoch 72, loss 0.6950\n",
      "Epoch 73, loss 0.6948\n",
      "Epoch 74, loss 0.6941\n",
      "Epoch 75, loss 0.6940\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 76, loss 0.6936\n",
      "Epoch 77, loss 0.6934\n",
      "Epoch 78, loss 0.6929\n",
      "Epoch 79, loss 0.6928\n",
      "Epoch 80, loss 0.6925\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 81, loss 0.6920\n",
      "Epoch 82, loss 0.6917\n",
      "Epoch 83, loss 0.6915\n",
      "Epoch 84, loss 0.6913\n",
      "Epoch 85, loss 0.6908\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 86, loss 0.6904\n",
      "Epoch 87, loss 0.6903\n",
      "Epoch 88, loss 0.6900\n",
      "Epoch 89, loss 0.6894\n",
      "Epoch 90, loss 0.6895\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 91, loss 0.6890\n",
      "Epoch 92, loss 0.6882\n",
      "Epoch 93, loss 0.6882\n",
      "Epoch 94, loss 0.6883\n",
      "Epoch 95, loss 0.6877\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 96, loss 0.6869\n",
      "Epoch 97, loss 0.6869\n",
      "Epoch 98, loss 0.6867\n",
      "Epoch 99, loss 0.6863\n",
      "Epoch 100, loss 0.6855\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 101, loss 0.6856\n",
      "Epoch 102, loss 0.6849\n",
      "Epoch 103, loss 0.6849\n",
      "Epoch 104, loss 0.6840\n",
      "Epoch 105, loss 0.6840\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 106, loss 0.6836\n",
      "Epoch 107, loss 0.6835\n",
      "Epoch 108, loss 0.6824\n",
      "Epoch 109, loss 0.6823\n",
      "Epoch 110, loss 0.6821\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 111, loss 0.6818\n",
      "Epoch 112, loss 0.6807\n",
      "Epoch 113, loss 0.6809\n",
      "Epoch 114, loss 0.6805\n",
      "Epoch 115, loss 0.6801\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 116, loss 0.6791\n",
      "Epoch 117, loss 0.6790\n",
      "Epoch 118, loss 0.6786\n",
      "Epoch 119, loss 0.6783\n",
      "Epoch 120, loss 0.6774\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 121, loss 0.6769\n",
      "Epoch 122, loss 0.6766\n",
      "Epoch 123, loss 0.6760\n",
      "Epoch 124, loss 0.6757\n",
      "Epoch 125, loss 0.6755\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 126, loss 0.6743\n",
      "Epoch 127, loss 0.6738\n",
      "Epoch 128, loss 0.6741\n",
      "Epoch 129, loss 0.6728\n",
      "Epoch 130, loss 0.6719\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 131, loss 0.6723\n",
      "Epoch 132, loss 0.6713\n",
      "Epoch 133, loss 0.6705\n",
      "Epoch 134, loss 0.6704\n",
      "Epoch 135, loss 0.6692\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 136, loss 0.6691\n",
      "Epoch 137, loss 0.6681\n",
      "Epoch 138, loss 0.6675\n",
      "Epoch 139, loss 0.6672\n",
      "Epoch 140, loss 0.6664\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 141, loss 0.6658\n",
      "Epoch 142, loss 0.6653\n",
      "Epoch 143, loss 0.6640\n",
      "Epoch 144, loss 0.6636\n",
      "Epoch 145, loss 0.6628\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 146, loss 0.6623\n",
      "Epoch 147, loss 0.6608\n",
      "Epoch 148, loss 0.6605\n",
      "Epoch 149, loss 0.6600\n",
      "Epoch 150, loss 0.6587\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 151, loss 0.6580\n",
      "Epoch 152, loss 0.6570\n",
      "Epoch 153, loss 0.6565\n",
      "Epoch 154, loss 0.6552\n",
      "Epoch 155, loss 0.6546\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 156, loss 0.6536\n",
      "Epoch 157, loss 0.6525\n",
      "Epoch 158, loss 0.6515\n",
      "Epoch 159, loss 0.6502\n",
      "Epoch 160, loss 0.6497\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 161, loss 0.6483\n",
      "Epoch 162, loss 0.6477\n",
      "Epoch 163, loss 0.6456\n",
      "Epoch 164, loss 0.6459\n",
      "Epoch 165, loss 0.6440\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 166, loss 0.6428\n",
      "Epoch 167, loss 0.6410\n",
      "Epoch 168, loss 0.6415\n",
      "Epoch 169, loss 0.6386\n",
      "Epoch 170, loss 0.6384\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 171, loss 0.6355\n",
      "Epoch 172, loss 0.6417\n",
      "Epoch 173, loss 0.6351\n",
      "Epoch 174, loss 0.6324\n",
      "Epoch 175, loss 0.6315\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 176, loss 0.6298\n",
      "Epoch 177, loss 0.6279\n",
      "Epoch 178, loss 0.6276\n",
      "Epoch 179, loss 0.6246\n",
      "Epoch 180, loss 0.6245\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 181, loss 0.6221\n",
      "Epoch 182, loss 0.6208\n",
      "Epoch 183, loss 0.6190\n",
      "Epoch 184, loss 0.6175\n",
      "Epoch 185, loss 0.6165\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 186, loss 0.6137\n",
      "Epoch 187, loss 0.6128\n",
      "Epoch 188, loss 0.6104\n",
      "Epoch 189, loss 0.6090\n",
      "Epoch 190, loss 0.6072\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 191, loss 0.6043\n",
      "Epoch 192, loss 0.6031\n",
      "Epoch 193, loss 0.6009\n",
      "Epoch 194, loss 0.5997\n",
      "Epoch 195, loss 0.5966\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 196, loss 0.5952\n",
      "Epoch 197, loss 0.5981\n",
      "Epoch 198, loss 0.5929\n",
      "Epoch 199, loss 0.5961\n",
      "Epoch 200, loss 0.5905\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 201, loss 0.5829\n",
      "Epoch 202, loss 0.5840\n",
      "Epoch 203, loss 0.5789\n",
      "Epoch 204, loss 0.5779\n",
      "Epoch 205, loss 0.5763\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 206, loss 0.5722\n",
      "Epoch 207, loss 0.5709\n",
      "Epoch 208, loss 0.5688\n",
      "Epoch 209, loss 0.5655\n",
      "Epoch 210, loss 0.5635\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 211, loss 0.5604\n",
      "Epoch 212, loss 0.5587\n",
      "Epoch 213, loss 0.5556\n",
      "Epoch 214, loss 0.5528\n",
      "Epoch 215, loss 0.5503\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 216, loss 0.5481\n",
      "Epoch 217, loss 0.5454\n",
      "Epoch 218, loss 0.5418\n",
      "Epoch 219, loss 0.5392\n",
      "Epoch 220, loss 0.5368\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 221, loss 0.5333\n",
      "Epoch 222, loss 0.5308\n",
      "Epoch 223, loss 0.5275\n",
      "Epoch 224, loss 0.5277\n",
      "Epoch 225, loss 0.5229\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 226, loss 0.5184\n",
      "Epoch 227, loss 0.5160\n",
      "Epoch 228, loss 0.5129\n",
      "Epoch 229, loss 0.5093\n",
      "Epoch 230, loss 0.5068\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 231, loss 0.5030\n",
      "Epoch 232, loss 0.5004\n",
      "Epoch 233, loss 0.4961\n",
      "Epoch 234, loss 0.4947\n",
      "Epoch 235, loss 0.4896\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236, loss 0.4881\n",
      "Epoch 237, loss 0.4833\n",
      "Epoch 238, loss 0.4807\n",
      "Epoch 239, loss 0.4770\n",
      "Epoch 240, loss 0.4781\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 241, loss 0.4741\n",
      "Epoch 242, loss 0.4643\n",
      "Epoch 243, loss 0.4649\n",
      "Epoch 244, loss 0.4589\n",
      "Epoch 245, loss 0.4572\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 246, loss 0.4517\n",
      "Epoch 247, loss 0.4504\n",
      "Epoch 248, loss 0.4459\n",
      "Epoch 249, loss 0.4417\n",
      "Epoch 250, loss 0.4394\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 251, loss 0.4349\n",
      "Epoch 252, loss 0.4314\n",
      "Epoch 253, loss 0.4276\n",
      "Epoch 254, loss 0.4251\n",
      "Epoch 255, loss 0.4208\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 256, loss 0.4167\n",
      "Epoch 257, loss 0.4141\n",
      "Epoch 258, loss 0.4094\n",
      "Epoch 259, loss 0.4065\n",
      "Epoch 260, loss 0.4016\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 261, loss 0.3990\n",
      "Epoch 262, loss 0.3959\n",
      "Epoch 263, loss 0.3910\n",
      "Epoch 264, loss 0.3881\n",
      "Epoch 265, loss 0.3835\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 266, loss 0.3804\n",
      "Epoch 267, loss 0.3756\n",
      "Epoch 268, loss 0.3728\n",
      "Epoch 269, loss 0.3690\n",
      "Epoch 270, loss 0.3657\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 271, loss 0.3614\n",
      "Epoch 272, loss 0.3585\n",
      "Epoch 273, loss 0.3541\n",
      "Epoch 274, loss 0.3513\n",
      "Epoch 275, loss 0.3464\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 276, loss 0.3438\n",
      "Epoch 277, loss 0.3398\n",
      "Epoch 278, loss 0.3363\n",
      "Epoch 279, loss 0.3327\n",
      "Epoch 280, loss 0.3288\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 281, loss 0.3263\n",
      "Epoch 282, loss 0.3222\n",
      "Epoch 283, loss 0.3184\n",
      "Epoch 284, loss 0.3151\n",
      "Epoch 285, loss 0.3124\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 286, loss 0.3085\n",
      "Epoch 287, loss 0.3035\n",
      "Epoch 288, loss 0.3021\n",
      "Epoch 289, loss 0.2977\n",
      "Epoch 290, loss 0.2943\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 291, loss 0.2910\n",
      "Epoch 292, loss 0.2885\n",
      "Epoch 293, loss 0.2841\n",
      "Epoch 294, loss 0.2810\n",
      "Epoch 295, loss 0.2779\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 296, loss 0.2746\n",
      "Epoch 297, loss 0.2720\n",
      "Epoch 298, loss 0.2682\n",
      "Epoch 299, loss 0.2649\n",
      "Epoch 300, loss 0.2629\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 301, loss 0.2588\n",
      "Epoch 302, loss 0.2557\n",
      "Epoch 303, loss 0.2530\n",
      "Epoch 304, loss 0.2503\n",
      "Epoch 305, loss 0.2476\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 306, loss 0.2435\n",
      "Epoch 307, loss 0.2408\n",
      "Epoch 308, loss 0.2381\n",
      "Epoch 309, loss 0.2355\n",
      "Epoch 310, loss 0.2327\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 311, loss 0.2289\n",
      "Epoch 312, loss 0.2267\n",
      "Epoch 313, loss 0.2243\n",
      "Epoch 314, loss 0.2213\n",
      "Epoch 315, loss 0.2190\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 316, loss 0.2163\n",
      "Epoch 317, loss 0.2135\n",
      "Epoch 318, loss 0.2111\n",
      "Epoch 319, loss 0.2085\n",
      "Epoch 320, loss 0.2056\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 321, loss 0.2037\n",
      "Epoch 322, loss 0.2010\n",
      "Epoch 323, loss 0.1983\n",
      "Epoch 324, loss 0.1963\n",
      "Epoch 325, loss 0.1937\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 326, loss 0.1917\n",
      "Epoch 327, loss 0.1894\n",
      "Epoch 328, loss 0.1866\n",
      "Epoch 329, loss 0.1847\n",
      "Epoch 330, loss 0.1827\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 331, loss 0.1802\n",
      "Epoch 332, loss 0.1783\n",
      "Epoch 333, loss 0.1760\n",
      "Epoch 334, loss 0.1741\n",
      "Epoch 335, loss 0.1722\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 336, loss 0.1693\n",
      "Epoch 337, loss 0.1681\n",
      "Epoch 338, loss 0.1656\n",
      "Epoch 339, loss 0.1640\n",
      "Epoch 340, loss 0.1622\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 341, loss 0.1601\n",
      "Epoch 342, loss 0.1583\n",
      "Epoch 343, loss 0.1567\n",
      "Epoch 344, loss 0.1549\n",
      "Epoch 345, loss 0.1530\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 346, loss 0.1509\n",
      "Epoch 347, loss 0.1492\n",
      "Epoch 348, loss 0.1478\n",
      "Epoch 349, loss 0.1461\n",
      "Epoch 350, loss 0.1442\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 351, loss 0.1426\n",
      "Epoch 352, loss 0.1409\n",
      "Epoch 353, loss 0.1394\n",
      "Epoch 354, loss 0.1377\n",
      "Epoch 355, loss 0.1365\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 356, loss 0.1354\n",
      "Epoch 357, loss 0.1336\n",
      "Epoch 358, loss 0.1318\n",
      "Epoch 359, loss 0.1308\n",
      "Epoch 360, loss 0.1288\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 361, loss 0.1278\n",
      "Epoch 362, loss 0.1258\n",
      "Epoch 363, loss 0.1247\n",
      "Epoch 364, loss 0.1237\n",
      "Epoch 365, loss 0.1219\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 366, loss 0.1207\n",
      "Epoch 367, loss 0.1197\n",
      "Epoch 368, loss 0.1178\n",
      "Epoch 369, loss 0.1168\n",
      "Epoch 370, loss 0.1156\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 371, loss 0.1143\n",
      "Epoch 372, loss 0.1130\n",
      "Epoch 373, loss 0.1117\n",
      "Epoch 374, loss 0.1108\n",
      "Epoch 375, loss 0.1094\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 376, loss 0.1085\n",
      "Epoch 377, loss 0.1076\n",
      "Epoch 378, loss 0.1062\n",
      "Epoch 379, loss 0.1049\n",
      "Epoch 380, loss 0.1039\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 381, loss 0.1033\n",
      "Epoch 382, loss 0.1017\n",
      "Epoch 383, loss 0.1007\n",
      "Epoch 384, loss 0.0998\n",
      "Epoch 385, loss 0.0987\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 386, loss 0.0976\n",
      "Epoch 387, loss 0.0965\n",
      "Epoch 388, loss 0.0955\n",
      "Epoch 389, loss 0.0946\n",
      "Epoch 390, loss 0.0938\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 391, loss 0.0928\n",
      "Epoch 392, loss 0.0918\n",
      "Epoch 393, loss 0.0909\n",
      "Epoch 394, loss 0.0899\n",
      "Epoch 395, loss 0.0892\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 396, loss 0.0885\n",
      "Epoch 397, loss 0.0974\n",
      "Epoch 398, loss 0.0858\n",
      "Epoch 399, loss 0.0863\n",
      "Epoch 400, loss 0.0848\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 401, loss 0.0839\n",
      "Epoch 402, loss 0.0836\n",
      "Epoch 403, loss 0.0821\n",
      "Epoch 404, loss 0.0816\n",
      "Epoch 405, loss 0.0806\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 406, loss 0.0800\n",
      "Epoch 407, loss 0.0792\n",
      "Epoch 408, loss 0.0785\n",
      "Epoch 409, loss 0.0775\n",
      "Epoch 410, loss 0.0771\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 411, loss 0.0761\n",
      "Epoch 412, loss 0.0755\n",
      "Epoch 413, loss 0.0749\n",
      "Epoch 414, loss 0.0743\n",
      "Epoch 415, loss 0.0734\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 416, loss 0.0727\n",
      "Epoch 417, loss 0.0721\n",
      "Epoch 418, loss 0.0711\n",
      "Epoch 419, loss 0.0708\n",
      "Epoch 420, loss 0.0702\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 421, loss 0.0695\n",
      "Epoch 422, loss 0.0687\n",
      "Epoch 423, loss 0.0683\n",
      "Epoch 424, loss 0.0677\n",
      "Epoch 425, loss 0.0669\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 426, loss 0.0664\n",
      "Epoch 427, loss 0.0658\n",
      "Epoch 428, loss 0.0652\n",
      "Epoch 429, loss 0.0645\n",
      "Epoch 430, loss 0.0641\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 431, loss 0.0633\n",
      "Epoch 432, loss 0.0628\n",
      "Epoch 433, loss 0.0623\n",
      "Epoch 434, loss 0.0619\n",
      "Epoch 435, loss 0.0613\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 436, loss 0.0609\n",
      "Epoch 437, loss 0.0599\n",
      "Epoch 438, loss 0.0602\n",
      "Epoch 439, loss 0.0592\n",
      "Epoch 440, loss 0.0589\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 441, loss 0.0580\n",
      "Epoch 442, loss 0.0578\n",
      "Epoch 443, loss 0.0569\n",
      "Epoch 444, loss 0.0567\n",
      "Epoch 445, loss 0.0561\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 446, loss 0.0558\n",
      "Epoch 447, loss 0.0551\n",
      "Epoch 448, loss 0.0552\n",
      "Epoch 449, loss 0.0541\n",
      "Epoch 450, loss 0.0541\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 451, loss 0.0532\n",
      "Epoch 452, loss 0.0533\n",
      "Epoch 453, loss 0.0523\n",
      "Epoch 454, loss 0.0526\n",
      "Epoch 455, loss 0.0514\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 456, loss 0.0517\n",
      "Epoch 457, loss 0.0506\n",
      "Epoch 458, loss 0.0510\n",
      "Epoch 459, loss 0.0499\n",
      "Epoch 460, loss 0.0505\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 461, loss 0.0492\n",
      "Epoch 462, loss 0.0503\n",
      "Epoch 463, loss 0.0487\n",
      "Epoch 464, loss 0.0502\n",
      "Epoch 465, loss 0.0484\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 466, loss 0.0510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 467, loss 0.0491\n",
      "Epoch 468, loss 0.0532\n",
      "Epoch 469, loss 0.0510\n",
      "Epoch 470, loss 0.0569\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 471, loss 0.0538\n",
      "Epoch 472, loss 0.0610\n",
      "Epoch 473, loss 0.0586\n",
      "Epoch 474, loss 0.0653\n",
      "Epoch 475, loss 0.0607\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 476, loss 0.0648\n",
      "Epoch 477, loss 0.0591\n",
      "Epoch 478, loss 0.0644\n",
      "Epoch 479, loss 0.0599\n",
      "Epoch 480, loss 0.0635\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 481, loss 0.0594\n",
      "Epoch 482, loss 0.0652\n",
      "Epoch 483, loss 0.0602\n",
      "Epoch 484, loss 0.0648\n",
      "Epoch 485, loss 0.0612\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 486, loss 0.0647\n",
      "Epoch 487, loss 0.0592\n",
      "Epoch 488, loss 0.0648\n",
      "Epoch 489, loss 0.0618\n",
      "Epoch 490, loss 0.0653\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 491, loss 0.0629\n",
      "Epoch 492, loss 0.0645\n",
      "Epoch 493, loss 0.0608\n",
      "Epoch 494, loss 0.0656\n",
      "Epoch 495, loss 0.0658\n",
      "Accuracy of argmax predictions on the training set: 100.000000%\n",
      "Epoch 496, loss 0.0634\n",
      "Epoch 497, loss 0.0604\n",
      "Epoch 498, loss 0.0657\n",
      "Epoch 499, loss 0.0656\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "#model = Classifier(1, 256, trainset.num_classes)\n",
    "model = Classifier(1, 256, 2)\n",
    "model.cuda()\n",
    "loss_func = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "\n",
    "labels_shuffled = torch.LongTensor(labels_shuffled).cuda()\n",
    "\n",
    "epoch_losses = []\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for iter, bg in enumerate(trainset_shuffled):\n",
    "        prediction=torch.zeros(1,2,dtype=torch.float64).cuda()\n",
    "        prediction[0] = model(bg)\n",
    "\n",
    "        loss = loss_func(prediction, labels_shuffled[iter])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        #add in gradient clipping:\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "    epoch_loss /= (iter + 1)\n",
    "    print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    \n",
    "    if (epoch%5==0):\n",
    "        model.eval()\n",
    "        eval_bg = dgl.batch(trainset_shuffled)\n",
    "        eval_labels = torch.tensor(labels_shuffled).float().view(-1, 1)\n",
    "        probs_Y = torch.softmax(model(eval_bg), 1)\n",
    "        argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1)\n",
    "\n",
    "        print('Accuracy of argmax predictions on the training set: {:4f}%'.format(\n",
    "            (eval_labels == argmax_Y.float()).sum().item() / len(eval_labels) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curve of a run is presented below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('cross entropy averaged over minibatches')\n",
    "plt.plot(epoch_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Testset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64]\n",
      "[30 39 63]\n"
     ]
    }
   ],
   "source": [
    "testset = []\n",
    "test_labels = []\n",
    "test_dir = 'test_data'\n",
    "\n",
    "testing_idx=[]\n",
    "for i in range(4,65):\n",
    "    if i not in training_idx:\n",
    "        testing_idx.append(i)\n",
    "\n",
    "print(testing_idx)\n",
    "print(training_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1]]\n"
     ]
    }
   ],
   "source": [
    "for idx in testing_idx:\n",
    "    node_list=[]\n",
    "    edge_list=[]\n",
    "    label_list=[]\n",
    "    node_list2=[]\n",
    "    edge_list2=[]\n",
    "    label_list2=[]\n",
    "    node_list3=[]\n",
    "    edge_list3=[]\n",
    "    label_list3=[]\n",
    "    node_list4=[]\n",
    "    edge_list4=[]\n",
    "    label_list4=[]\n",
    "    for j in [\"node_list\",\"edge_list\",\"graph_label\"]:\n",
    "        filename = \"cla_\"+str(idx)+\"bit\"+j+'.csv'\n",
    "        filename2 = \"CSkipA_\"+str(idx)+\"bit\"+j+'.csv'\n",
    "        \n",
    "        if(filename.find(\"node_list\")>=0):\n",
    "            with open(test_dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                node_list = list(reader)\n",
    "                \n",
    "        if(filename.find(\"edge_list\")>=0):\n",
    "            with open(test_dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                edge_list = list(reader)\n",
    "        if(filename.find(\"graph_label\")>=0):\n",
    "            with open(test_dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                label_list = list(reader)\n",
    "        if(filename.find(\"gate_type\")>=0):\n",
    "            with open(test_dir+'/'+filename, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                gate_type = list(reader)\n",
    "        \n",
    "        if(filename2.find(\"node_list\")>=0):\n",
    "            with open(test_dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                node_list2 = list(reader)\n",
    "                \n",
    "        if(filename2.find(\"edge_list\")>=0):\n",
    "            with open(test_dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                edge_list2 = list(reader)\n",
    "        if(filename2.find(\"graph_label\")>=0):\n",
    "            with open(test_dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                label_list2 = list(reader)\n",
    "        if(filename2.find(\"gate_type\")>=0):\n",
    "            with open(test_dir+'/'+filename2, 'rt') as fh:\n",
    "                reader=csv.reader(fh)\n",
    "                gate_type2 = list(reader)\n",
    "    #create dgl graph\n",
    "    g=build_circuit_graph_undirected(node_list,edge_list)\n",
    "    testset.append(g)\n",
    "    test_labels.append(label_list[0])\n",
    "    g2=build_circuit_graph_undirected(node_list2,edge_list2)\n",
    "    testset.append(g2)\n",
    "    test_labels.append(['1'])\n",
    "\n",
    "\n",
    "for i in test_labels:\n",
    "    i[0] = int(i[0])\n",
    "\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10  74  66  51   7  78  92  90 107  62  30   2  94  22  43  84  24  73\n",
      "  60  56  16  13  26  86  71  97 111 100   8  93  33  45   3  48   6  54\n",
      " 115  95 101  96  91  63  27  18  11  59  68  61  76  50   1 104  42  41\n",
      "   4  15  17  52  40  38   5  53 106 108   0  34  28  55  35  23  31  75\n",
      "  57  89  99  32  98  14  85  19  29  49  82 113 114  79  69  80  20 109\n",
      "  72  77  25  37  81 102  46 105  39  65  58  12 110  88  70  87  36  21\n",
      "  83   9 103 112  67  64  47  44]\n"
     ]
    }
   ],
   "source": [
    "##apply random shuffle on the testset   \n",
    "np.random.seed(0)\n",
    "randomize = np.arange(len(testset))\n",
    "np.random.shuffle(randomize)\n",
    "testset_shuffled=[]\n",
    "test_labels_shuffled=[]\n",
    "for i in range (len(randomize)):\n",
    "    test_labels_shuffled.append(test_labels[randomize[i]])\n",
    "    testset_shuffled.append(testset[randomize[i]])\n",
    "print(randomize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([116, 1])\n"
     ]
    }
   ],
   "source": [
    "print(argmax_Y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of sampled predictions on the test set: 92.2414%\n",
      "Accuracy of argmax predictions on the test set: 100.000000%\n",
      "tensor([0.9296, 0.0704], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.9324, 0.0676], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7627, 0.2373], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0077, 0.9923], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0116, 0.9884], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.9275, 0.0725], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.9308, 0.0692], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.9392, 0.0608], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0051, 0.9949], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7678, 0.2322], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.8571, 0.1429], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.9843, 0.0157], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.9361, 0.0639], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.8255, 0.1745], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0084, 0.9916], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7707, 0.2293], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7826, 0.2174], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0064, 0.9936], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.9214, 0.0786], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.9195, 0.0805], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.8182, 0.1818], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0127, 0.9873], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.8629, 0.1371], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7801, 0.2199], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0064, 0.9936], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0054, 0.9946], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0049, 0.9951], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7494, 0.2506], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.8752, 0.1248], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0057, 0.9943], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0099, 0.9901], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0084, 0.9916], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0116, 0.9884], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.9207, 0.0793], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.9731, 0.0269], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.9078, 0.0922], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0049, 0.9951], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0054, 0.9946], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0054, 0.9946], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.9278, 0.0722], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0057, 0.9943], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0072, 0.9928], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0099, 0.9901], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.8478, 0.1522], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0127, 0.9873], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0072, 0.9928], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.9293, 0.0707], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0072, 0.9928], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7731, 0.2269], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.9319, 0.0681], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0278, 0.9722], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7541, 0.2459], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.8352, 0.1648], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0091, 0.9909], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.9480, 0.0520], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0127, 0.9873], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0127, 0.9873], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.9265, 0.0735], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.8213, 0.1787], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.8601, 0.1399], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0116, 0.9884], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0077, 0.9923], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.9300, 0.0700], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7512, 0.2488], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.9633, 0.0367], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.8462, 0.1538], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.8151, 0.1849], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0077, 0.9923], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0091, 0.9909], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0109, 0.9891], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0099, 0.9901], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0064, 0.9936], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0072, 0.9928], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0057, 0.9943], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0054, 0.9946], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.8061, 0.1939], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.9332, 0.0668], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.9037, 0.0963], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0060, 0.9940], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0109, 0.9891], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0099, 0.9901], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0084, 0.9916], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.9285, 0.0715], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0049, 0.9951], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.9439, 0.0561], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0060, 0.9940], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0068, 0.9932], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7659, 0.2341], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7983, 0.2017], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0051, 0.9949], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.9254, 0.0746], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0064, 0.9936], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0109, 0.9891], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0091, 0.9909], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0060, 0.9940], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.9293, 0.0707], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.8248, 0.1752], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0051, 0.9949], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0091, 0.9909], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0068, 0.9932], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7610, 0.2390], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.8068, 0.1932], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7594, 0.2406], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7668, 0.2332], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.9362, 0.0638], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0057, 0.9943], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.8169, 0.1831], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0109, 0.9891], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0060, 0.9940], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0116, 0.9884], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.0051, 0.9949], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7486, 0.2514], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0068, 0.9932], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7805, 0.2195], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.0084, 0.9916], device='cuda:0', grad_fn=<SelectBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.8111, 0.1889], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_bg = dgl.batch(testset_shuffled)\n",
    "test_labels = torch.tensor(test_labels_shuffled).float().view(-1, 1).cuda()\n",
    "probs_Y = torch.softmax(model(test_bg), 1)\n",
    "\n",
    "sampled_Y = torch.multinomial(probs_Y, 1)\n",
    "argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1)\n",
    "print('Accuracy of sampled predictions on the test set: {:.4f}%'.format(\n",
    "    (test_labels == sampled_Y.float()).sum().item() / len(test_labels) * 100))\n",
    "print('Accuracy of argmax predictions on the test set: {:4f}%'.format(\n",
    "    (test_labels == argmax_Y.float()).sum().item() / len(test_labels) * 100))\n",
    "\n",
    "zip(model(test_bg),(test_labels))\n",
    "for i1,i2 in zip(probs_Y,(test_labels)):\n",
    "    print(i1,i2)\n",
    "# print(torch.max(probs_Y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsne\n",
    "\n",
    "#m = trainset[0].adjacency_matrix()\n",
    "m = model(test_bg).cpu().data.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing the data using PCA...\n",
      "Computing pairwise distances...\n",
      "Computing P-values for point 0 of 116...\n",
      "Mean value of sigma: 0.296336\n",
      "Iteration 10: error is 9.028207\n",
      "Iteration 20: error is 8.641590\n",
      "Iteration 30: error is 9.796931\n",
      "Iteration 40: error is 9.088763\n",
      "Iteration 50: error is 9.126360\n",
      "Iteration 60: error is 8.927308\n",
      "Iteration 70: error is 8.736146\n",
      "Iteration 80: error is 8.550334\n",
      "Iteration 90: error is 8.490677\n",
      "Iteration 100: error is 8.488299\n",
      "Iteration 110: error is 0.964325\n",
      "Iteration 120: error is 0.668839\n",
      "Iteration 130: error is 0.485058\n",
      "Iteration 140: error is 0.267286\n",
      "Iteration 150: error is 0.110439\n",
      "Iteration 160: error is 0.067484\n",
      "Iteration 170: error is 0.046610\n",
      "Iteration 180: error is 0.043015\n",
      "Iteration 190: error is 0.042008\n",
      "Iteration 200: error is 0.041902\n",
      "Iteration 210: error is 0.041786\n",
      "Iteration 220: error is 0.041675\n",
      "Iteration 230: error is 0.041547\n",
      "Iteration 240: error is 0.041421\n",
      "Iteration 250: error is 0.041294\n",
      "Iteration 260: error is 0.041174\n",
      "Iteration 270: error is 0.041066\n",
      "Iteration 280: error is 0.040970\n",
      "Iteration 290: error is 0.040869\n",
      "Iteration 300: error is 0.040781\n",
      "Iteration 310: error is 0.040693\n",
      "Iteration 320: error is 0.040603\n",
      "Iteration 330: error is 0.040529\n",
      "Iteration 340: error is 0.040466\n",
      "Iteration 350: error is 0.040379\n",
      "Iteration 360: error is 0.040319\n",
      "Iteration 370: error is 0.040256\n",
      "Iteration 380: error is 0.040192\n",
      "Iteration 390: error is 0.040127\n",
      "Iteration 400: error is 0.040066\n",
      "Iteration 410: error is 0.040006\n",
      "Iteration 420: error is 0.039950\n",
      "Iteration 430: error is 0.039893\n",
      "Iteration 440: error is 0.039840\n",
      "Iteration 450: error is 0.039789\n",
      "Iteration 460: error is 0.039738\n",
      "Iteration 470: error is 0.039690\n",
      "Iteration 480: error is 0.039643\n",
      "Iteration 490: error is 0.039599\n",
      "Iteration 500: error is 0.039557\n",
      "Iteration 510: error is 0.039516\n",
      "Iteration 520: error is 0.039479\n",
      "Iteration 530: error is 0.039444\n",
      "Iteration 540: error is 0.039405\n",
      "Iteration 550: error is 0.039370\n",
      "Iteration 560: error is 0.039334\n",
      "Iteration 570: error is 0.039299\n",
      "Iteration 580: error is 0.039265\n",
      "Iteration 590: error is 0.039233\n",
      "Iteration 600: error is 0.039199\n",
      "Iteration 610: error is 0.039170\n",
      "Iteration 620: error is 0.039142\n",
      "Iteration 630: error is 0.039113\n",
      "Iteration 640: error is 0.039083\n",
      "Iteration 650: error is 0.039054\n",
      "Iteration 660: error is 0.039026\n",
      "Iteration 670: error is 0.038999\n",
      "Iteration 680: error is 0.038973\n",
      "Iteration 690: error is 0.038947\n",
      "Iteration 700: error is 0.038923\n",
      "Iteration 710: error is 0.038898\n",
      "Iteration 720: error is 0.038875\n",
      "Iteration 730: error is 0.038855\n",
      "Iteration 740: error is 0.038833\n",
      "Iteration 750: error is 0.038809\n",
      "Iteration 760: error is 0.038789\n",
      "Iteration 770: error is 0.038767\n",
      "Iteration 780: error is 0.038747\n",
      "Iteration 790: error is 0.038728\n",
      "Iteration 800: error is 0.038707\n",
      "Iteration 810: error is 0.038689\n",
      "Iteration 820: error is 0.038668\n",
      "Iteration 830: error is 0.038650\n",
      "Iteration 840: error is 0.038632\n",
      "Iteration 850: error is 0.038613\n",
      "Iteration 860: error is 0.038595\n",
      "Iteration 870: error is 0.038577\n",
      "Iteration 880: error is 0.038559\n",
      "Iteration 890: error is 0.038542\n",
      "Iteration 900: error is 0.038524\n",
      "Iteration 910: error is 0.038509\n",
      "Iteration 920: error is 0.038494\n",
      "Iteration 930: error is 0.038476\n",
      "Iteration 940: error is 0.038461\n",
      "Iteration 950: error is 0.038446\n",
      "Iteration 960: error is 0.038430\n",
      "Iteration 970: error is 0.038415\n",
      "Iteration 980: error is 0.038401\n",
      "Iteration 990: error is 0.038386\n",
      "Iteration 1000: error is 0.038372\n"
     ]
    }
   ],
   "source": [
    "Y = tsne.tsne(m, 2, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 13.4228394  -11.49259042]\n",
      " [-10.21815194  14.23977827]\n",
      " [  8.43736586 -15.24965017]\n",
      " [  8.78063637 -15.76906534]\n",
      " [  9.27310843 -14.73426781]\n",
      " [  8.40072757 -15.2754915 ]\n",
      " [-10.59810131  13.85570675]\n",
      " [ 12.78680992 -11.54748612]\n",
      " [  8.93477065 -15.56504416]\n",
      " [ 12.26983167 -12.38527972]\n",
      " [-12.51148492  11.9262152 ]\n",
      " [  9.55700061 -14.75301034]\n",
      " [-13.06505887  11.36833234]\n",
      " [ -9.52968842  15.02966089]\n",
      " [  8.3702075  -15.30340927]\n",
      " [ 14.23574809 -10.71807669]\n",
      " [-11.54574185  12.89988869]\n",
      " [ -9.90346911  14.55621915]\n",
      " [ -8.68757923  15.42845312]\n",
      " [  9.54863858 -14.85499713]\n",
      " [  9.54501187 -14.86053335]\n",
      " [  8.47866222 -15.87555294]\n",
      " [ 12.30528034 -12.35644408]\n",
      " [  9.7280896  -14.53890422]\n",
      " [  8.88728147 -15.64558992]\n",
      " [ -9.29156731  15.64513958]\n",
      " [  8.26430005 -15.49070259]\n",
      " [-10.21775002  14.23979359]\n",
      " [ 13.34288083 -11.74265156]\n",
      " [ 12.91721116 -11.47363963]\n",
      " [-13.59945858  10.83951361]\n",
      " [  9.26806281 -15.12627084]\n",
      " [ -9.04622642  15.04507402]\n",
      " [ -9.52968873  15.02966257]\n",
      " [-10.59827579  13.85536988]\n",
      " [-14.03876602  10.42734893]\n",
      " [-13.26553399  11.16796579]\n",
      " [ -8.68759314  15.42861042]\n",
      " [-13.59980449  10.8400907 ]\n",
      " [ -9.43504238  14.93691302]\n",
      " [-13.06485294  11.36876952]\n",
      " [ -8.70007188  15.53654194]\n",
      " [-13.26610436  11.16778143]\n",
      " [-11.98797044  12.45428207]\n",
      " [-10.21772399  14.23982   ]\n",
      " [ 13.24284406 -11.92218241]\n",
      " [ 12.6533299  -11.69610592]\n",
      " [-12.51159349  11.92580962]\n",
      " [-12.51147329  11.92536489]\n",
      " [ -9.0462276   15.04507318]\n",
      " [-11.98686786  12.45441268]\n",
      " [-12.51145816  11.92585116]\n",
      " [  9.29002669 -15.1138682 ]\n",
      " [-13.06494144  11.36794762]\n",
      " [ -8.82346226  15.18297978]\n",
      " [-10.59925131  13.85397062]\n",
      " [ 12.68655964 -11.6469998 ]\n",
      " [  8.8626112  -15.68062618]\n",
      " [  9.31837631 -14.7092216 ]\n",
      " [ -9.04920676  15.7841811 ]\n",
      " [  8.84537732 -15.0834994 ]\n",
      " [-13.0655435   11.36839429]\n",
      " [ -8.82346013  15.18298921]\n",
      " [ 12.88282105 -11.48866264]\n",
      " [ 12.55813258 -12.13407606]\n",
      " [  9.56305181 -14.64788045]\n",
      " [ 12.59551655 -12.06612834]\n",
      " [  8.72649391 -15.81001133]\n",
      " [ 14.13023188 -10.80556909]\n",
      " [  8.26314764 -15.77077857]\n",
      " [  9.658559   -14.59254958]\n",
      " [-13.26552176  11.16797799]\n",
      " [ -9.90350677  14.55618039]\n",
      " [ 13.52015161 -11.33074622]\n",
      " [  8.38761994 -15.85085398]\n",
      " [ 10.21822391 -14.12846277]\n",
      " [  8.58958838 -15.17527443]\n",
      " [ -8.94129437  15.77293943]\n",
      " [  9.17742397 -14.81904865]\n",
      " [ -9.9035328   14.55615328]\n",
      " [ 13.05707095 -12.0804355 ]\n",
      " [ -9.42673162  15.42017105]\n",
      " [-11.9868751   12.45464083]\n",
      " [ -9.04935819  15.78419154]\n",
      " [ 12.64927786 -11.70303992]\n",
      " [-10.59874822  13.85518607]\n",
      " [ 12.5889892  -11.99031994]\n",
      " [ 13.0284135  -11.44076449]\n",
      " [ 13.16003123 -12.01244191]\n",
      " [-10.21772399  14.23982   ]\n",
      " [ -9.43504344  14.93691329]\n",
      " [ 13.10543747 -12.05336382]\n",
      " [ 13.24224036 -11.9230863 ]\n",
      " [-11.06028185  13.38912229]\n",
      " [-11.54536383  12.9004119 ]\n",
      " [  8.31416259 -15.37473092]\n",
      " [ -8.70003308  15.53553484]\n",
      " [ 10.16748607 -14.17181783]\n",
      " [  8.27253827 -15.76794768]\n",
      " [  9.90783144 -14.39128603]\n",
      " [ 10.04616465 -14.27494569]\n",
      " [ -9.2912099   15.6448706 ]\n",
      " [-11.06057995  13.38930879]\n",
      " [ -9.42673205  15.42017045]\n",
      " [-11.5456179   12.89922408]\n",
      " [ 12.94369619 -12.11840487]\n",
      " [ 12.8991576  -12.12449325]\n",
      " [ 10.39875852 -13.97503457]\n",
      " [-11.54571588  12.89958291]\n",
      " [ 13.18875254 -11.43428115]\n",
      " [-11.98688442  12.45463137]\n",
      " [ 13.66990554 -11.19097435]\n",
      " [-13.59980631  10.84008889]\n",
      " [-11.06053764  13.3889804 ]\n",
      " [ -8.94131467  15.77294819]\n",
      " [ -9.90347029  14.5562179 ]\n",
      " [ 11.274595   -13.2303    ]\n",
      " [ 14.02176822 -10.89787614]\n",
      " [-13.2660664   11.16780208]\n",
      " [-13.59968716  10.83977364]]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2,2)\n",
    "\n",
    "color = test_labels.cpu()*85/255\n",
    "for i in range (len(test_labels)):\n",
    "    axs[0,0].scatter(m[i,0],m[i,1],color=(0,color[i],0))\n",
    "    axs[0,1].scatter(Y[i,0],Y[i,1],color=(0,color[i],0))\n",
    "   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "265.99px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
